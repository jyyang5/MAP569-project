{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn import metrics\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "# import xgboost as xgb\n",
    "# from xgboost import plot_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id_Customer</th>\n",
       "      <th>Y</th>\n",
       "      <th>Number_Of_Dependant</th>\n",
       "      <th>Years_At_Residence</th>\n",
       "      <th>Years_At_Business</th>\n",
       "      <th>Nb_Of_Products</th>\n",
       "      <th>Net_Annual_Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7440</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id_Customer  Y  Number_Of_Dependant  Years_At_Residence  Years_At_Business  \\\n",
       "0         7440  0                  3.0                   1                1.0   \n",
       "1          573  0                  0.0                  12                2.0   \n",
       "\n",
       "   Nb_Of_Products Net_Annual_Income  \n",
       "0               1                36  \n",
       "1               1                18  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('https://raw.githubusercontent.com/jyyang5/MAP569-project/master/CreditTraining.csv')\n",
    "label_df = data_df['Prod_Category'].tolist()\n",
    "\n",
    "# divide into categorical data and else as clean\n",
    "clean_df = data_df.select_dtypes(exclude=['object']).copy()\n",
    "cat_df = data_df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# replace comma with dot \n",
    "temp_list = []\n",
    "for ele in data_df['Net_Annual_Income'].tolist():\n",
    "  if type(ele) is not float:\n",
    "    temp_list.append(ele.replace(',','.'))\n",
    "  else:\n",
    "    temp_list.append(ele)\n",
    "\n",
    "clean_df['Net_Annual_Income'] = temp_list\n",
    "del cat_df['Net_Annual_Income']\n",
    "clean_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: there are categorical and timestamps that is not numeric.\n",
    "\n",
    "## 1.1. Deal with categorical data (timestamp data excluded)\n",
    "\n",
    "From all categorical datas ['Customer_Type', 'BirthDate', 'Customer_Open_Date', 'P_Client',\n",
    "       'Educational_Level', 'Marital_Status', 'Net_Annual_Income',\n",
    "       'Prod_Sub_Category', 'Prod_Decision_Date', 'Source',\n",
    "       'Type_Of_Residence', 'Prod_Closed_Date', 'Prod_Category']\n",
    "\n",
    "We first exclude ['BirthDate', 'Customer_Open_Date', 'Net_Annual_Income', 'Prod_Decision_Date']\n",
    "\n",
    "We use 0-1 encoding for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Existing Client    3369\n",
      "Existing Client        2011\n",
      "Name: Customer_Type, dtype: int64\n",
      "NP_Client    4968\n",
      "P_Client      412\n",
      "Name: P_Client, dtype: int64\n",
      "University           4785\n",
      "Master/PhD            522\n",
      "Diploma                58\n",
      "Secondary or Less      15\n",
      "Name: Educational_Level, dtype: int64\n",
      "Married      4206\n",
      "Single       1046\n",
      "Widowed        64\n",
      "Divorced       63\n",
      "Separated       1\n",
      "Name: Marital_Status, dtype: int64\n",
      "C    4638\n",
      "G     624\n",
      "P     118\n",
      "Name: Prod_Sub_Category, dtype: int64\n",
      "Sales     4119\n",
      "Branch    1261\n",
      "Name: Source, dtype: int64\n",
      "Owned       4791\n",
      "Old rent     323\n",
      "Parents      179\n",
      "New rent      83\n",
      "Company        4\n",
      "Name: Type_Of_Residence, dtype: int64\n",
      "B    3176\n",
      "D     670\n",
      "C     517\n",
      "K     265\n",
      "L     236\n",
      "G     188\n",
      "E     101\n",
      "H      79\n",
      "J      71\n",
      "M      49\n",
      "A      19\n",
      "F       5\n",
      "I       4\n",
      "Name: Prod_Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for name in ['Customer_Type', 'P_Client',\n",
    "            'Educational_Level', 'Marital_Status',\n",
    "            'Prod_Sub_Category', 'Source',\n",
    "            'Type_Of_Residence', 'Prod_Category']:\n",
    "        print(cat_df[name].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'Customer_Type', 'P_Client', 'Source' all have just two types we therefore restrict ourselves to binary variable.\n",
    "\n",
    "$$\n",
    "Customer\\_Type = \n",
    " \\begin{cases}\n",
    "1& \\text{Existing Client} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "P\\_Client = \n",
    " \\begin{cases}\n",
    "1& \\text{P Client} \\\\\n",
    "0 & \\text{NP Client}\n",
    "\\end{cases} \\\\\n",
    "Source= \n",
    " \\begin{cases}\n",
    "1& \\text{Sales} \\\\\n",
    "0 & \\text{Brranch}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id_Customer</th>\n",
       "      <th>Y</th>\n",
       "      <th>Number_Of_Dependant</th>\n",
       "      <th>Years_At_Residence</th>\n",
       "      <th>Years_At_Business</th>\n",
       "      <th>Nb_Of_Products</th>\n",
       "      <th>Net_Annual_Income</th>\n",
       "      <th>Educational_Level_Diploma</th>\n",
       "      <th>Educational_Level_Master/PhD</th>\n",
       "      <th>Educational_Level_Secondary or Less</th>\n",
       "      <th>...</th>\n",
       "      <th>Prod_Category_G</th>\n",
       "      <th>Prod_Category_H</th>\n",
       "      <th>Prod_Category_I</th>\n",
       "      <th>Prod_Category_J</th>\n",
       "      <th>Prod_Category_K</th>\n",
       "      <th>Prod_Category_L</th>\n",
       "      <th>Prod_Category_M</th>\n",
       "      <th>Customer_Type</th>\n",
       "      <th>P_Client</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7440</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id_Customer  Y  Number_Of_Dependant  Years_At_Residence  Years_At_Business  \\\n",
       "0         7440  0                  3.0                   1                1.0   \n",
       "1          573  0                  0.0                  12                2.0   \n",
       "\n",
       "   Nb_Of_Products Net_Annual_Income  Educational_Level_Diploma  \\\n",
       "0               1                36                          0   \n",
       "1               1                18                          0   \n",
       "\n",
       "   Educational_Level_Master/PhD  Educational_Level_Secondary or Less  ...  \\\n",
       "0                             0                                    0  ...   \n",
       "1                             0                                    0  ...   \n",
       "\n",
       "   Prod_Category_G  Prod_Category_H  Prod_Category_I  Prod_Category_J  \\\n",
       "0                0                0                0                0   \n",
       "1                1                0                0                0   \n",
       "\n",
       "   Prod_Category_K  Prod_Category_L  Prod_Category_M  Customer_Type  P_Client  \\\n",
       "0                0                0                0              0         1   \n",
       "1                0                0                0              1         0   \n",
       "\n",
       "   Source  \n",
       "0       0  \n",
       "1       0  \n",
       "\n",
       "[2 rows x 40 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add categorical data [timestamp data not added]\n",
    "clean_df1 = clean_df.join(pd.get_dummies(cat_df[['Customer_Type', 'P_Client',\n",
    "                                                'Educational_Level', 'Marital_Status',\n",
    "                                                'Prod_Sub_Category', 'Source',\n",
    "                                                'Type_Of_Residence', 'Prod_Category']]))\n",
    "\n",
    "clean_df1['Customer_Type'] = clean_df1['Customer_Type_Existing Client']\n",
    "del clean_df1['Customer_Type_Existing Client']\n",
    "del clean_df1['Customer_Type_Non Existing Client']\n",
    "\n",
    "clean_df1['P_Client'] = clean_df1['P_Client_NP_Client']\n",
    "del clean_df1['P_Client_NP_Client']\n",
    "del clean_df1['P_Client_P_Client']\n",
    "\n",
    "clean_df1['Source'] = clean_df1['Source_Branch']\n",
    "del clean_df1['Source_Branch']\n",
    "del clean_df1['Source_Sales']\n",
    "\n",
    "\n",
    "clean_df1[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Deal with timestamps \n",
    "**Appraoches**: We use duration so that the variable is comparable \n",
    "\n",
    "['BirthDate', 'Customer_Open_Date', 'Net_Annual_Income', 'Prod_Decision_Date', 'Prod_Closed_Date']\n",
    "\n",
    "\n",
    "- `Birth_Duration = Now - BirthDate` ('BirthDate'): assuming there is a distribution of credibility, just started working -> less credit, worked for a long time but not close to retirement -> high credit) \n",
    "- `Customer_Open_Duration = Now - Customer_Open_Date` ('Customer_Open_Date'): usually longer the history is, the more royal the customer is\n",
    "- 'Prod_Closed_Date' - 'Prod_Decision_Date'\n",
    "    - length of the product?: if product closed \n",
    "    - `Prod_not_closed = (Prod_closed != nan)`: dummy variable if the product is closed 'Prod_not_closed' = 0 if closed\n",
    "    - `Prod_Decision_Duration = Now - Prod_Decision_Date`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 634, 1879, 1987, 2750, 5045, 5144]), array([1, 5, 3, 3, 1, 5]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# duration: birth - now\n",
    "temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in cat_df['BirthDate'].tolist()]\n",
    "clean_df1['Birth_Duration'] = temp_list\n",
    "\n",
    "# duration: Customer_Open_Date\n",
    "temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in cat_df['Customer_Open_Date'].tolist()]\n",
    "clean_df1['Customer_Open_Duration'] = temp_list\n",
    "\n",
    "# dummy var: product closed = 1\n",
    "temp_list = [int(type(ele) != float) for ele in cat_df['Prod_Closed_Date'].tolist()]\n",
    "clean_df1['Prod_not_closed'] = temp_list\n",
    "\n",
    "# duration: Prod_Decision_Date\n",
    "temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in cat_df['Prod_Decision_Date'].tolist()]\n",
    "clean_df1['Prod_Decision_Duration'] = temp_list\n",
    "\n",
    "# convert to a list \n",
    "data_list = []\n",
    "labels = []\n",
    "for name in clean_df1.columns.values:\n",
    "    if name is 'Id_Customer':\n",
    "        pass\n",
    "    elif name is 'Y':\n",
    "        labels = clean_df1[name].tolist()\n",
    "    else:\n",
    "        data_list.append(clean_df1[name].tolist())\n",
    "\n",
    "data_list = np.transpose(data_list)\n",
    "data_list = np.array(data_list, dtype=np.float64)\n",
    "np.where(np.isnan(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to training and testing, we check if there is nan element\n",
    "\n",
    "Customers with nan \n",
    "- Years_At_Business\n",
    "    - Id_Customer = 398, 5882\t\n",
    "- Number_Of_Dependant\n",
    "    - Id_Customer = 8953, 9588\t\n",
    "- Net_Annual_Income\n",
    "    - Id_Customer = 9399, 9555\t\n",
    "\n",
    "## 1.3. Nan elements \n",
    "There are two usual approaches to deal with nan elements, the first one drops the data with nan, the second on the other hand replace the nan element. Since it is very likely that we may face future clients with unkown data (nan), so that we use the second approach to make the method proposed more general.\n",
    "\n",
    "- Median \n",
    "- Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Check done: ', True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df2 = clean_df1.copy()\n",
    "\n",
    "for name in ['Number_Of_Dependant', 'Years_At_Business', 'Net_Annual_Income']:\n",
    "    temp_list = np.array(clean_df2[name].tolist(), dtype=np.float64)\n",
    "    temp_list[np.isnan(temp_list)] = np.median(temp_list[~np.isnan(temp_list)])\n",
    "    # print(sum(np.isnan(temp_list)))\n",
    "    clean_df2[name] = temp_list\n",
    "\n",
    "# convert to train mat \n",
    "data_list = []\n",
    "labels = []\n",
    "for name in clean_df2.columns.values:\n",
    "    if name == 'Id_Customer':\n",
    "        pass\n",
    "        \n",
    "    elif name is 'Y':\n",
    "        labels = clean_df2[name].tolist()\n",
    "    else:\n",
    "        data_list.append(clean_df2[name].tolist())\n",
    "\n",
    "data_list = np.transpose(data_list)\n",
    "data_list = np.array(data_list, dtype=np.float64)\n",
    "np.where(np.isnan(data_list))\n",
    "train_df = clean_df2.copy()\n",
    "del train_df['Y']\n",
    "del train_df['Id_Customer']\n",
    "'Check done: ', np.shape(train_df)== np.shape(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------all-----------\n",
      "0 4987\n",
      "1 393\n",
      "percentage of 1: 0.07304832713754647\n",
      "-----------train-----------\n",
      "0 3751\n",
      "1 284\n",
      "percentage of 1: 0.07038413878562577\n",
      "-----------test-----------\n",
      "0 1236\n",
      "1 109\n",
      "percentage of 1: 0.08104089219330855\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_list, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# np.save('X_train.npy',X_train)\n",
    "# np.save('X_test.npy',X_test)\n",
    "# np.save('y_train.npy',y_train)\n",
    "# np.save('y_test.npy',y_test)\n",
    "\n",
    "print('-----------all-----------')\n",
    "for i in set(labels):\n",
    "    print(i,labels.count(i))\n",
    "print('percentage of 1:', labels.count(1)/len(labels))\n",
    "\n",
    "print('-----------train-----------')\n",
    "for i in set(labels):\n",
    "    print(i,y_train.count(i))\n",
    "print('percentage of 1:', y_train.count(1)/len(y_train))\n",
    "\n",
    "print('-----------test-----------')\n",
    "for i in set(labels):\n",
    "    print(i,y_test.count(i))\n",
    "print('percentage of 1:', y_test.count(1)/len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to be unbalanced (two labels has a relative big difference in percentag), we first try without balancing the data.\n",
    "\n",
    "## 2.1. Evaluation metrics\n",
    "\n",
    "| \t\t|     same clusters|   different clusters|\n",
    "| :-------- | --------:| :------: |\n",
    "| same class    |   TP  |  FN |\n",
    "|different class|   FP  |  TN |\n",
    "- $P(precision) = \\frac{TP}{TP+FP}$ \n",
    "- $R(recall) = \\frac{TP}{TP+FN}$ \n",
    "- $F_{\\beta}= \\frac{(\\beta^2+1)PR}{\\beta^2P+R}$\n",
    "\n",
    "### Note: since label=1 means that the client has defaulted on its credit, which is something that we definately want to avoid, we focus on *R(recall)* (percentage of detected clients among all truly defaulted clients) . In order to taken into consideration the *P(precision)* (the percentage of true clients among all detected clients), we use the *F 0.5-score*, the * $F_\\beta$-score where $\\beta = 2$, as the evaluation of the model\n",
    "\n",
    "$\\color{red}{\\text{From now on, the precison, recall and F-2 score are referred to prediction with label=1}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.2. Primal Test with Basic Model: SVM, KNN & Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_metric(y_test, preds):\n",
    "    \"\"\" \n",
    "    print the classification report \n",
    "    \"\"\"\n",
    "    print('confusion matrix')\n",
    "    print(confusion_matrix(y_test, preds))\n",
    "    print('summary [label=1]')\n",
    "    beta = 2\n",
    "    res = precision_recall_fscore_support(y_test, preds, beta = beta, pos_label = 1, average = 'binary')\n",
    "    print(\"precision:{}\\nrecall:{}\\nsupport:{}\".format(round(res[0],3),round(res[1],3),res[3]))\n",
    "    print('-------')\n",
    "    print(\"accuracy:\",round(accuracy_score(y_test, preds),3))\n",
    "    print(\"F{}-score:{}\".format(beta,round(res[2],3)))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds, pos_label=2)\n",
    "    metrics.auc(fpr, tpr)\n",
    "\n",
    "def tests(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" print the evaluation results of deterministic models\n",
    "    \"\"\"\n",
    "    # KNeighbors\n",
    "    neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    pred_KNN = neigh.predict(X_test)\n",
    "    print('----------------K-Neighbors-------------------')\n",
    "    eval_metric(y_test, pred_KNN)\n",
    "\n",
    "    # SVM\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_SVM = clf.predict(X_test)\n",
    "    print('----------------SVM-------------------')\n",
    "    eval_metric(y_test, pred_SVM)\n",
    "    \n",
    "    #Decision Tree\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    dt.fit(X_train, y_train)\n",
    "    pred_DT = dt.predict(X_test)\n",
    "    print('-----------------Decision Tree------------------')\n",
    "    eval_metric(y_test, pred_DT)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------K-Neighbors-------------------\n",
      "confusion matrix\n",
      "[[1230    6]\n",
      " [ 107    2]]\n",
      "summary [label=1]\n",
      "precision:0.25\n",
      "recall:0.018\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.916\n",
      "F2-score:0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------SVM-------------------\n",
      "confusion matrix\n",
      "[[1236    0]\n",
      " [ 109    0]]\n",
      "summary [label=1]\n",
      "precision:0.0\n",
      "recall:0.0\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.919\n",
      "F2-score:0.0\n",
      "-----------------Decision Tree------------------\n",
      "confusion matrix\n",
      "[[1181   55]\n",
      " [  57   52]]\n",
      "summary [label=1]\n",
      "precision:0.486\n",
      "recall:0.477\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.917\n",
      "F2-score:0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "tests(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- The accuracy of the three basic models reaches 0.9, which is satisfaisant for other ML tasks. However, we can notice that the SVM and KNN models predict that all the labels are '0', thanks to the imbalance of the data, the accuracy is still high with this sort of prediction, but it is not what we want. This is one reason why we chose to focus on Recall, Precision and F-score instead of Accuracy.\n",
    "- The rebalancing of input data is necessary espacially for models such as SVM, KNN and LR \n",
    "- The F2-score of Decision Tree reaches 0.47 with imbalanced data, this shows that the Decision Tree, as well as random forest, which will be used in the next part, could work well on imbalanced data.\n",
    "\n",
    "## 2.3. Balance data\n",
    "In Section 2.2, we did some experiments but find that even if the overall accurancy is high, the prediction results of clients with label=1 is not good. Especially with classifiers like SVM, the class with more samples (majority class) is favoured. We therefore resample the minority class to balance the train data and see how the result might improve. Since the size of the minority class is very small, we w.l.o.g. use Upsampling.   \n",
    "\n",
    "# 3. Encapsulation \n",
    "We combine all mentioned, add a few more functionalities and create the class below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Credit_predictor():\n",
    "    def __init__(self,path):\n",
    "        \"\"\"\n",
    "        read the data\n",
    "        \"\"\"\n",
    "        self.raw_data = pd.read_csv(path)\n",
    "        self.clean_df = self.raw_data.select_dtypes(exclude=['object']).copy()\n",
    "        self.cat_df = self.raw_data.select_dtypes(include=['object']).copy()\n",
    "        # categorical data \n",
    "        self.cates = ['Customer_Type', 'P_Client',\n",
    "            'Educational_Level', 'Marital_Status',\n",
    "            'Prod_Sub_Category', 'Source',\n",
    "            'Type_Of_Residence', 'Prod_Category']\n",
    "        # all timestamp data \n",
    "        self.date_trans_set = {'Birth_Duration':'BirthDate',\n",
    "             'Customer_Open_Duration':'Customer_Open_Date',\n",
    "             'Prod_Decision_Duration':'Prod_Decision_Date'}\n",
    "        \n",
    "    def data_preprocessing_0(self):\n",
    "        \"\"\"\n",
    "        seperate the data into numerical set and categorical set\n",
    "        \"\"\"\n",
    "        self.clean_df = self.raw_data.select_dtypes(exclude=['object']).copy()\n",
    "        self.cat_df = self.raw_data.select_dtypes(include=['object']).copy()\n",
    "        temp_list = []\n",
    "        # convert value from comma to dot \n",
    "        for ele in self.raw_data['Net_Annual_Income'].tolist():\n",
    "            if type(ele) is not float:\n",
    "                temp_list.append(ele.replace(',','.'))\n",
    "            else:\n",
    "                temp_list.append(ele)\n",
    "        self.clean_df['Net_Annual_Income'] = temp_list\n",
    "        del self.cat_df['Net_Annual_Income']\n",
    "        self.dates_transformer()\n",
    "        \n",
    "    def dates_transformer(self):\n",
    "        \"\"\"\n",
    "        processing for the date\n",
    "        \"\"\"\n",
    "        # convert timestamp to duration: now - timestamp \n",
    "        for i in self.date_trans_set:\n",
    "            temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in self.cat_df[self.date_trans_set[i]].tolist()]\n",
    "            self.clean_df[i] = temp_list\n",
    "            del self.cat_df[self.date_trans_set[i]]\n",
    "        # indicator whether product is closed \n",
    "        temp_list = [int(type(ele) != float) for ele in self.cat_df['Prod_Closed_Date'].tolist()]\n",
    "        self.clean_df['Prod_not_closed'] = temp_list\n",
    "        del self.cat_df['Prod_Closed_Date']\n",
    "\n",
    "    def data_preprocessing_simple(self):\n",
    "        self.data_simple = self.clean_df.copy()\n",
    "        for i in self.cates:\n",
    "            labelencoder = LabelEncoder()\n",
    "            labelencoder.fit(self.cat_df[i])\n",
    "            self.data_simple[i] = labelencoder.transform(self.cat_df[i])\n",
    "        \n",
    "    def data_preprocessing_onehot(self):\n",
    "        binary = ['Customer_Type', 'P_Client', 'Source']\n",
    "        no_binary = [i for i in self.cates if i not in binary]\n",
    "        self.data_oh = self.clean_df.copy()\n",
    "        for i in binary:\n",
    "            labelencoder = LabelEncoder()\n",
    "            labelencoder.fit(self.cat_df[i])\n",
    "            \"\"\"\n",
    "            To see the representation of the labels : list(labelencoder.classes_)\n",
    "            \"\"\"\n",
    "            self.data_oh[i] = labelencoder.transform(self.cat_df[i])\n",
    "        self.data_oh = self.data_oh.join(pd.get_dummies(self.cat_df[no_binary]))\n",
    "        \n",
    "    def split_dataset(self,dataset):\n",
    "        X = dataset.drop('Y',axis = 1)\n",
    "        Y = dataset.Y\n",
    "        try:\n",
    "            imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "            imp_mean.fit(X)\n",
    "        except:\n",
    "            imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "            imp_mean.fit(X)\n",
    "        '''\n",
    "        SimpleImputer:\n",
    "        strategy : mean, median, most_frequent, constant\n",
    "        '''\n",
    "        X_imputed = imp_mean.transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_imputed, Y, test_size=0.25, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    def split_dataset_4_imbal(self,dataset):\n",
    "        X_train, X_test, y_train, y_test = self.split_dataset(dataset)\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "   #     ros = RandomOverSampler(random_state=0)\n",
    "   #     X_test, y_test = ros.fit_resample(X_test, y_test)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    \n",
    "    def convert_labels(self,labels):\n",
    "        \"\"\" \n",
    "        convert the input (0/1) labels to what is needed \n",
    "        return a binary label sequence \n",
    "        \"\"\"\n",
    "        labels = np.array(labels)\n",
    "        if sum(labels) > len(labels)/2:\n",
    "            labels[labels==1] = -1\n",
    "            labels[labels==0] = 1\n",
    "            labels[labels==-1] = 0\n",
    "        return labels\n",
    "\n",
    "    def eval_metric(self,y_test, preds):\n",
    "        \"\"\" \n",
    "        print the classification report \n",
    "        \"\"\"\n",
    "        print('confusion matrix')\n",
    "        print(confusion_matrix(y_test, preds))\n",
    "        print('--summary[label=1]--')\n",
    "     #   print('summary')\n",
    "        beta = 2\n",
    "        res = precision_recall_fscore_support(y_test, preds, beta = beta, pos_label = 1, average = 'binary')\n",
    "        print(\"precision:{}\\nrecall:{}\\nsupport:{}\".format(round(res[0],3),round(res[1],3),res[3]))\n",
    "        print('--------------------')\n",
    "        print(\"accuracy:\",round(accuracy_score(y_test, preds),3))\n",
    "        print(\"F{}-score:{}\".format(beta,round(res[2],3)))\n",
    "        print(\"\\n\")\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, preds, pos_label=2)\n",
    "        metrics.auc(fpr, tpr)   \n",
    "\n",
    "        \n",
    "    def model_eval(self,model,Normalize = False):\n",
    "        \"\"\"\n",
    "        print the evaluation result of the model with the Cp object \n",
    "        \"\"\"\n",
    "        self.data_preprocessing_0()\n",
    "        self.data_preprocessing_simple()\n",
    "        self.data_preprocessing_onehot()\n",
    "        \n",
    "        # one-hot\n",
    "        X_train, X_test, y_train, y_test = Cp.split_dataset(Cp.data_oh)\n",
    "        # one-hot [balanced]\n",
    "        X_train_b, X_test_b, y_train_b, y_test_b = Cp.split_dataset_4_imbal(Cp.data_oh)\n",
    "        # categorical -> 0:n_classes-1\n",
    "        X_train_t, X_test_t, y_train_t, y_test_t = Cp.split_dataset(Cp.data_simple)\n",
    "        # categorical -> 0:n_classes-1 [balanced]\n",
    "        X_train_tb, X_test_tb, y_train_tb, y_test_tb = Cp.split_dataset_4_imbal(Cp.data_simple)\n",
    "        \n",
    "        if Normalize:\n",
    "            X_train = normalize(X_train, norm='l2')\n",
    "            X_test = normalize(X_test, norm='l2')\n",
    "            \n",
    "        print('----------------one-hot-------------------')\n",
    "        model.fit(X_train, y_train)\n",
    "        pred_y = model.predict(X_test)\n",
    "        self.eval_metric(y_test, pred_y)\n",
    "        print('----------------one-hot[balanced]-------------------')\n",
    "        model.fit(X_train_b, y_train_b)\n",
    "        pred_y_b = model.predict(X_test_b)\n",
    "        self.eval_metric(y_test_b, pred_y_b)\n",
    "        print('----------------0:n_classes-1-------------------')\n",
    "        model.fit(X_train_t, y_train_t)\n",
    "        pred_y_t = model.predict(X_test_t)\n",
    "        Cp.eval_metric(y_test_t, pred_y_t)\n",
    "        print('----------------0:n_classes-1[balanced]-------------------')\n",
    "        model.fit(X_train_tb, y_train_tb)\n",
    "        pred_y_tb = model.predict(X_test_tb)\n",
    "        Cp.eval_metric(y_test_tb, pred_y_tb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'CreditTraining.csv'\n",
    "Cp = Credit_predictor(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Model selection \n",
    "### 3.1.1 LogisticRegression\n",
    "The *liblinear* method is chosen as it applies a coordinate descent (CD) algorithm, which performs better in a categarical data.\n",
    "\n",
    "The result of this model shows that the resampling of the data affect much the result for Logistic regression, in particular the *precision*, *recall*, and *F2-score*. Weather using onehot embedding or not does not affect much the result, which is the same case for all the other tests.\n",
    "\n",
    "The Logistic Regression Model reaches satisfactory recall and F2 score with balanced data, but low precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------one-hot-------------------\n",
      "confusion matrix\n",
      "[[1229    7]\n",
      " [  89   20]]\n",
      "--summary[label=1]--\n",
      "precision:0.741\n",
      "recall:0.183\n",
      "support:None\n",
      "--------------------\n",
      "accuracy: 0.929\n",
      "F2-score:0.216\n",
      "\n",
      "\n",
      "----------------one-hot[balanced]-------------------\n",
      "confusion matrix\n",
      "[[1109  127]\n",
      " [  16   93]]\n",
      "--summary[label=1]--\n",
      "precision:0.423\n",
      "recall:0.853\n",
      "support:None\n",
      "--------------------\n",
      "accuracy: 0.894\n",
      "F2-score:0.709\n",
      "\n",
      "\n",
      "----------------0:n_classes-1-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1223   13]\n",
      " [  81   28]]\n",
      "--summary[label=1]--\n",
      "precision:0.683\n",
      "recall:0.257\n",
      "support:None\n",
      "--------------------\n",
      "accuracy: 0.93\n",
      "F2-score:0.294\n",
      "\n",
      "\n",
      "----------------0:n_classes-1[balanced]-------------------\n",
      "confusion matrix\n",
      "[[1112  124]\n",
      " [  16   93]]\n",
      "--summary[label=1]--\n",
      "precision:0.429\n",
      "recall:0.853\n",
      "support:None\n",
      "--------------------\n",
      "accuracy: 0.896\n",
      "F2-score:0.712\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "Cp.model_eval(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Random Forest \n",
    "The rebalancing of the data affects not that much the *precision*, *recall*, and *F2-score* as Logistic Regression. Still, we can observe that the resampling could augment the *recall* and *F2-score*, but sacrifice the *precision*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------one-hot-------------------\n",
      "confusion matrix\n",
      "[[1228    8]\n",
      " [  76   33]]\n",
      "-------\n",
      "precision:0.805\n",
      "recall:0.303\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.938\n",
      "F2-score:0.346\n",
      "\n",
      "\n",
      "----------------one-hot[balanced]-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1205   31]\n",
      " [  60   49]]\n",
      "-------\n",
      "precision:0.612\n",
      "recall:0.45\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.932\n",
      "F2-score:0.475\n",
      "\n",
      "\n",
      "----------------0:n_classes-1-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1222   14]\n",
      " [  69   40]]\n",
      "-------\n",
      "precision:0.741\n",
      "recall:0.367\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.938\n",
      "F2-score:0.408\n",
      "\n",
      "\n",
      "----------------0:n_classes-1[balanced]-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1197   39]\n",
      " [  57   52]]\n",
      "-------\n",
      "precision:0.571\n",
      "recall:0.477\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.929\n",
      "F2-score:0.493\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=100)\n",
    "Cp.model_eval(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. SVM\n",
    "The linear kernel is applied, as it gives the best result for SVM, but the calculation time is sacrified. The normalization of the data does not affect much the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------one-hot-------------------\n",
      "confusion matrix\n",
      "[[1236    0]\n",
      " [ 107    2]]\n",
      "-------\n",
      "precision:1.0\n",
      "recall:0.018\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.92\n",
      "F2-score:0.023\n",
      "\n",
      "\n",
      "----------------one-hot[balanced]-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[818 418]\n",
      " [ 12  97]]\n",
      "-------\n",
      "precision:0.188\n",
      "recall:0.89\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.68\n",
      "F2-score:0.51\n",
      "\n",
      "\n",
      "----------------0:n_classes-1-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1235    1]\n",
      " [ 101    8]]\n",
      "-------\n",
      "precision:0.889\n",
      "recall:0.073\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.924\n",
      "F2-score:0.09\n",
      "\n",
      "\n",
      "----------------0:n_classes-1[balanced]-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[911 325]\n",
      " [ 15  94]]\n",
      "-------\n",
      "precision:0.224\n",
      "recall:0.862\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.747\n",
      "F2-score:0.55\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma='auto',kernel='linear')\n",
    "Cp.model_eval(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. LDA\n",
    "The LDA(Linear Discriminant Analysis) model gives high *recall* but comparatively low *precision*, the rebalancing of the data and the use of onehot embedding does not affect much the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------one-hot-------------------\n",
      "confusion matrix\n",
      "[[1134  102]\n",
      " [  21   88]]\n",
      "-------\n",
      "precision:0.463\n",
      "recall:0.807\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.909\n",
      "F2-score:0.703\n",
      "\n",
      "\n",
      "----------------one-hot[balanced]-------------------\n",
      "confusion matrix\n",
      "[[1107  129]\n",
      " [  13   96]]\n",
      "-------\n",
      "precision:0.427\n",
      "recall:0.881\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.894\n",
      "F2-score:0.726\n",
      "\n",
      "\n",
      "----------------0:n_classes-1-------------------\n",
      "confusion matrix\n",
      "[[1124  112]\n",
      " [  18   91]]\n",
      "-------\n",
      "precision:0.448\n",
      "recall:0.835\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.903\n",
      "F2-score:0.712\n",
      "\n",
      "\n",
      "----------------0:n_classes-1[balanced]-------------------\n",
      "confusion matrix\n",
      "[[1108  128]\n",
      " [  13   96]]\n",
      "-------\n",
      "precision:0.429\n",
      "recall:0.881\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.895\n",
      "F2-score:0.727\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "Cp.model_eval(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5. XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBClassifier(objective ='reg:logistic')\n",
    "model_eval(xg_reg, Cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_tweedie = xgb.XGBClassifier(objective ='binary:logitraw', eval_metric = 'error@0.5')\n",
    "model_eval(xg_tweedie, Cp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing objective and evaluation metric do not seem to have a huge difference.\n",
    "\n",
    "**Observations**\n",
    "- Balancing the data indeed helps to improve the model performance \n",
    "- We **select logistiRegression, LDA, and XGBoost given their $R_{\\text{label=1}}$ around 0.85, $P_{\\text{label=0}}=0.99$, $P_{\\text{label=1}}$ around 0.42 using the two balanced data.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Voting  \n",
    "\n",
    "In this section, we propose two voting methods that utilize the predictions of several models and hopes it would maximize the information obatined so far.\n",
    "- Improve precision of predicted defaulted\n",
    "- Improve coverage of truly defaulted\n",
    "\n",
    "## 4.1. Minimize innocent (Improve precision of predicted defaulted)\n",
    "\n",
    "Since we have three models **logistiRegression, LDA, and XGBoost** using balanced data have $R_{\\text{label=1}}$ around 0.85, meaning most of the defaulted customers are detected (which we think is a **relative good coverage**).\n",
    "\n",
    "But $\\text{label=1} \\approx 0.42$ , meaning more than half of the predicted is not defaulted, and it would be costly to do a thorough investigation of the users so we want to **minimize those innocent but detected**.   \n",
    "\n",
    "With 3 predictions from 3 individual parties, we want to extract the mutual information and believing that if one client is predictd defaulted by all parties then there is a larger probability that the one is defaulted compared with (1/3 of the parties voted so). And of course, we might lose some covergae. **We try taking a logical AND to do this**.\n",
    "\n",
    "**Essentially, we are doing a voting by setting the weight to be 100% iff. all parties vote 1, ensuring that the number of inocent client is minimized**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_tb, y_train_tb)\n",
    "lr_pred_y_tb = lr.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y_tb)\n",
    "\n",
    "xg_reg.fit(X_train_tb, y_train_tb)\n",
    "xg_pred_y_tb = xg_reg.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, xg_pred_y_tb)\n",
    "\n",
    "LDA.fit(X_train_tb, y_train_tb)\n",
    "LDA_pred_y_tb = LDA.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, LDA_pred_y_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cp.eval_metric(y_test_tb, LDA_pred_y_tb & xg_pred_y_tb & lr_pred_y_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: we did increase $P_{\\text{label=1}}$ to 0.44, but $R_{\\text{label=1}}$ drops to 0.84, the improvement of $P_{\\text{label=1}}$ is small, one potential explaination is that the three parties share a lot of mutual information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "parties_pred_mat = [LDA_pred_y_tb, xg_pred_y_tb, lr_pred_y_tb]\n",
    "distance.cdist(parties_pred_mat, parties_pred_mat, 'hamming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the *Minimize innocent* voting mechanism fail to improve a $P_{\\text{label=1}}$ a lot is largely due to the percentage of shared information is dominant for all three parties (around 1% difference).\n",
    "\n",
    "## 4.2. Add trustworthy information ( Improve coverage of truly defaulted)\n",
    "\n",
    "We use one of the three parties as a base (with good covergae) and then we add prediction by party with high $P_{\\text{label=1}}$ but possibly low coverage ($R_{\\text{label=1}}$), so as to improve precison and recall at the same time.\n",
    "\n",
    "**Essentially, we are doing a voting by setting the weight of trustworthy party to 100%, meaning once they vote 1, the client is 1 in prediction.**\n",
    "\n",
    "- Base\n",
    "    - `lr_pred_y_tb`: $P_{\\text{label=1}}=0.43, R_{\\text{label=1}}=0.88$ \n",
    "    - `xg_pred_y_tb`: $P_{\\text{label=1}}=0.43, R_{\\text{label=1}}=0.85$ \n",
    "- Trustworthy\n",
    "    - `lr_pred_y` : $P_{\\text{label=1}}=0.80, R_{\\text{label=1}}=0.15$ \n",
    "    - `rfc_pred_y` : $P_{\\text{label=1}}=0.76, R_{\\text{label=1}}=0.28$       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred_y = rfc.predict(X_test)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred_y = lr.predict(X_test)\n",
    "\n",
    "\n",
    "print('-------------------LDA+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | LDA_pred_y_tb)\n",
    "\n",
    "print('-------------------LDA+rfc-------------------')\n",
    "Cp.eval_metric(y_test_tb, rfc_pred_y | LDA_pred_y_tb)\n",
    "\n",
    "print('-------------------LDA+rfc+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | rfc_pred_y | LDA_pred_y_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------XG+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | xg_pred_y_tb)\n",
    "\n",
    "print('-------------------XG+rfc-------------------')\n",
    "Cp.eval_metric(y_test_tb, rfc_pred_y | xg_pred_y_tb)\n",
    "\n",
    "print('-------------------XG+rfc+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | rfc_pred_y | xg_pred_y_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, these *trustworthy information* seem to have been covered already by the parties with good coverage.\n",
    "\n",
    "# 5. Conclusion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
