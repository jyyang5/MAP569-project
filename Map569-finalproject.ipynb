{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Credit_predictor():\n",
    "    def __init__(self,path):\n",
    "        \"\"\"\n",
    "        read the data\n",
    "        \"\"\"\n",
    "        self.raw_data = pd.read_csv('CreditTraining.csv')\n",
    "        self.clean_df = self.raw_data.select_dtypes(exclude=['object']).copy()\n",
    "        self.cat_df = self.raw_data.select_dtypes(include=['object']).copy()\n",
    "        self.cates = ['Customer_Type', 'P_Client',\n",
    "            'Educational_Level', 'Marital_Status',\n",
    "            'Prod_Sub_Category', 'Source',\n",
    "            'Type_Of_Residence', 'Prod_Category']\n",
    "        self.date_trans_set = {'Birth_Duration':'BirthDate',\n",
    "             'Customer_Open_Duration':'Customer_Open_Date',\n",
    "             'Prod_Decision_Duration':'Prod_Decision_Date'}\n",
    "        \n",
    "    def data_preprocessing_0(self):\n",
    "        \"\"\"\n",
    "        seperate the data into numerical set and categorical set\n",
    "        \"\"\"\n",
    "        self.clean_df = self.raw_data.select_dtypes(exclude=['object']).copy()\n",
    "        self.cat_df = self.raw_data.select_dtypes(include=['object']).copy()\n",
    "        temp_list = []\n",
    "        for ele in self.raw_data['Net_Annual_Income'].tolist():\n",
    "            if type(ele) is not float:\n",
    "                temp_list.append(ele.replace(',','.'))\n",
    "            else:\n",
    "                temp_list.append(ele)\n",
    "\n",
    "        self.clean_df['Net_Annual_Income'] = temp_list\n",
    "        del self.cat_df['Net_Annual_Income']\n",
    "        self.dates_transformer()\n",
    "        \n",
    "    def dates_transformer(self):\n",
    "        \"\"\"\n",
    "        processing for the date\n",
    "        \"\"\"\n",
    "        for i in self.date_trans_set:\n",
    "            temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in self.cat_df[self.date_trans_set[i]].tolist()]\n",
    "            self.clean_df[i] = temp_list\n",
    "            del self.cat_df[self.date_trans_set[i]]\n",
    "        temp_list = [int(type(ele) != float) for ele in self.cat_df['Prod_Closed_Date'].tolist()]\n",
    "        self.clean_df['Prod_not_closed'] = temp_list\n",
    "        del self.cat_df['Prod_Closed_Date']\n",
    "\n",
    "    def data_preprocessing_simple(self):\n",
    "        self.data_simple = self.clean_df.copy()\n",
    "        for i in self.cates:\n",
    "            labelencoder = LabelEncoder()\n",
    "            labelencoder.fit(self.cat_df[i])\n",
    "            self.data_simple[i] = labelencoder.transform(self.cat_df[i])\n",
    "        \n",
    "    def data_preprocessing_onehot(self):\n",
    "        binary = ['Customer_Type', 'P_Client', 'Source']\n",
    "        no_binary = [i for i in self.cates if i not in binary]\n",
    "        self.data_oh = self.clean_df.copy()\n",
    "        for i in binary:\n",
    "            labelencoder = LabelEncoder()\n",
    "            labelencoder.fit(self.cat_df[i])\n",
    "            \"\"\"\n",
    "            To see the representation of the labels : list(labelencoder.classes_)\n",
    "            \"\"\"\n",
    "            self.data_oh[i] = labelencoder.transform(self.cat_df[i])\n",
    "        self.data_oh = self.data_oh.join(pd.get_dummies(self.cat_df[no_binary]))\n",
    "        \n",
    "    def split_dataset(self,dataset):\n",
    "        X = dataset.drop('Y',axis = 1)\n",
    "        Y = dataset.Y\n",
    "        try:\n",
    "            imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "            imp_mean.fit(X)\n",
    "        except:\n",
    "            imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "            imp_mean.fit(X)\n",
    "        '''\n",
    "        SimpleImputer:\n",
    "        strategy : mean, median, most_frequent, constant\n",
    "        '''\n",
    "        X_imputed = imp_mean.transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_imputed, Y, test_size=0.25, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def split_dataset_4_imbal(self,dataset):\n",
    "        X_train, X_test, y_train, y_test = self.split_dataset(dataset)\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "   #     ros = RandomOverSampler(random_state=0)\n",
    "   #     X_test, y_test = ros.fit_resample(X_test, y_test)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    \n",
    "    def convert_labels(self,labels):\n",
    "        \"\"\" \n",
    "        convert the input (0/1) labels to what is needed \n",
    "        return a binary label sequence \n",
    "        \"\"\"\n",
    "        labels = np.array(labels)\n",
    "        if sum(labels) > len(labels)/2:\n",
    "            labels[labels==1] = -1\n",
    "            labels[labels==0] = 1\n",
    "            labels[labels==-1] = 0\n",
    "        return labels\n",
    "\n",
    "   \n",
    "    def eval_metric(self, y_test, preds):\n",
    "        \"\"\" \n",
    "        print the classification report \n",
    "        \"\"\"\n",
    "     #   preds = convert_labels(preds)\n",
    "        print('confusion matrix')\n",
    "        print(confusion_matrix(y_test, preds))\n",
    "        print('\\n')\n",
    "        print('summary')\n",
    "        print(classification_report(y_test, preds))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, preds, pos_label=2)\n",
    "        metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'CreditTraining.csv'\n",
    "Cp = Credit_predictor(path)\n",
    "Cp.data_preprocessing_0()\n",
    "Cp.data_preprocessing_simple()\n",
    "Cp.data_preprocessing_onehot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = Cp.split_dataset(Cp.data_oh)\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = Cp.split_dataset_4_imbal(Cp.data_oh)\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = Cp.split_dataset(Cp.data_simple)\n",
    "X_train_tb, X_test_tb, y_train_tb, y_test_tb = Cp.split_dataset_4_imbal(Cp.data_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1228    8]\n",
      " [  89   20]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      1236\n",
      "           1       0.71      0.18      0.29       109\n",
      "\n",
      "    accuracy                           0.93      1345\n",
      "   macro avg       0.82      0.59      0.63      1345\n",
      "weighted avg       0.91      0.93      0.91      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "pred_y = lr.predict(X_test)\n",
    "Cp.eval_metric(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1113  123]\n",
      " [  16   93]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94      1236\n",
      "           1       0.43      0.85      0.57       109\n",
      "\n",
      "    accuracy                           0.90      1345\n",
      "   macro avg       0.71      0.88      0.76      1345\n",
      "weighted avg       0.94      0.90      0.91      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train_b, y_train_b)\n",
    "pred_y_b = lr.predict(X_test_b)\n",
    "Cp.eval_metric(y_test_b, pred_y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1112  124]\n",
      " [  16   93]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94      1236\n",
      "           1       0.43      0.85      0.57       109\n",
      "\n",
      "    accuracy                           0.90      1345\n",
      "   macro avg       0.71      0.88      0.76      1345\n",
      "weighted avg       0.94      0.90      0.91      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train_tb, y_train_tb)\n",
    "pred_y_tb = lr.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, pred_y_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1180   56]\n",
      " [  60   49]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1236\n",
      "           1       0.47      0.45      0.46       109\n",
      "\n",
      "    accuracy                           0.91      1345\n",
      "   macro avg       0.71      0.70      0.71      1345\n",
      "weighted avg       0.91      0.91      0.91      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train_t,y_train_t)\n",
    "pred_dtc = dtc.predict(X_test_t)\n",
    "Cp.eval_metric(y_test_t, pred_dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1178   58]\n",
      " [  58   51]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1236\n",
      "           1       0.47      0.47      0.47       109\n",
      "\n",
      "    accuracy                           0.91      1345\n",
      "   macro avg       0.71      0.71      0.71      1345\n",
      "weighted avg       0.91      0.91      0.91      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train,y_train)\n",
    "pred_dtc = dtc.predict(X_test)\n",
    "Cp.eval_metric(y_test, pred_dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1184   52]\n",
      " [  54   55]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1236\n",
      "           1       0.51      0.50      0.51       109\n",
      "\n",
      "    accuracy                           0.92      1345\n",
      "   macro avg       0.74      0.73      0.73      1345\n",
      "weighted avg       0.92      0.92      0.92      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train_b,y_train_b)\n",
    "pred_dtc_b = dtc.predict(X_test_b)\n",
    "Cp.eval_metric(y_test_b, pred_dtc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1229   80]\n",
      " [   7   29]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97      1309\n",
      "           1       0.27      0.81      0.40        36\n",
      "\n",
      "    accuracy                           0.94      1345\n",
      "   macro avg       0.63      0.87      0.68      1345\n",
      "weighted avg       0.97      0.94      0.95      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "# random forest model \n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train,y_train)\n",
    "# predictions\n",
    "pred_rfc = rfc.predict(X_test)\n",
    "\n",
    "Cp.eval_metric(pred_rfc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1204   58]\n",
      " [  32   51]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      1262\n",
      "           1       0.47      0.61      0.53        83\n",
      "\n",
      "    accuracy                           0.93      1345\n",
      "   macro avg       0.72      0.78      0.75      1345\n",
      "weighted avg       0.94      0.93      0.94      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_b,y_train_b)\n",
    "# predictions\n",
    "pred_rfc = rfc.predict(X_test_b)\n",
    "\n",
    "Cp.eval_metric(pred_rfc, y_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1211   25]\n",
      " [ 109    0]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      1236\n",
      "           1       0.00      0.00      0.00       109\n",
      "\n",
      "    accuracy                           0.90      1345\n",
      "   macro avg       0.46      0.49      0.47      1345\n",
      "weighted avg       0.84      0.90      0.87      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "KNN.fit(X_train, y_train)\n",
    "pred_KNN = KNN.predict(X_test)\n",
    "\n",
    "Cp.eval_metric(y_test, pred_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1057  179]\n",
      " [  91   18]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      1236\n",
      "           1       0.09      0.17      0.12       109\n",
      "\n",
      "    accuracy                           0.80      1345\n",
      "   macro avg       0.51      0.51      0.50      1345\n",
      "weighted avg       0.85      0.80      0.82      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "KNN.fit(X_train_b, y_train_b)\n",
    "pred_KNN_b = KNN.predict(X_test_b)\n",
    "\n",
    "Cp.eval_metric(y_test_b, pred_KNN_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1057  179]\n",
      " [  91   18]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      1236\n",
      "           1       0.09      0.17      0.12       109\n",
      "\n",
      "    accuracy                           0.80      1345\n",
      "   macro avg       0.51      0.51      0.50      1345\n",
      "weighted avg       0.85      0.80      0.82      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "KNN.fit(X_train_tb, y_train_tb)\n",
    "pred_KNN_tb = KNN.predict(X_test_tb)\n",
    "\n",
    "Cp.eval_metric(y_test_tb, pred_KNN_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1236    0]\n",
      " [ 109    0]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96      1236\n",
      "           1       0.00      0.00      0.00       109\n",
      "\n",
      "    accuracy                           0.92      1345\n",
      "   macro avg       0.46      0.50      0.48      1345\n",
      "weighted avg       0.84      0.92      0.88      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "pred_SVM = clf.predict(X_test)\n",
    "\n",
    "Cp.eval_metric(y_test, pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[660 576]\n",
      " [ 55  54]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.53      0.68      1236\n",
      "           1       0.09      0.50      0.15       109\n",
      "\n",
      "    accuracy                           0.53      1345\n",
      "   macro avg       0.50      0.51      0.41      1345\n",
      "weighted avg       0.86      0.53      0.63      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_n = normalize(X_train_b, norm='l2')\n",
    "X_test_n = normalize(X_test_b, norm='l2')\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train_n, y_train_b)\n",
    "pred_SVM_b = clf.predict(X_test_n)\n",
    "\n",
    "Cp.eval_metric(y_test_b, pred_SVM_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1107  129]\n",
      " [  13   96]]\n",
      "\n",
      "\n",
      "summary\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94      1236\n",
      "           1       0.43      0.88      0.57       109\n",
      "\n",
      "    accuracy                           0.89      1345\n",
      "   macro avg       0.71      0.89      0.76      1345\n",
      "weighted avg       0.94      0.89      0.91      1345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "LDA.fit(X_train_b, y_train_b)\n",
    "pred_LDA_b = LDA.predict(X_test_b)\n",
    "\n",
    "Cp.eval_metric(y_test_b, pred_LDA_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
