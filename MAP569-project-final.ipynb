{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "py37",
   "display_name": "Python (py37)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import model_selection\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Id_Customer  Y  Number_Of_Dependant  Years_At_Residence  Years_At_Business  \\\n0         7440  0                  3.0                   1                1.0   \n1          573  0                  0.0                  12                2.0   \n\n   Nb_Of_Products Net_Annual_Income  \n0               1                36  \n1               1                18  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id_Customer</th>\n      <th>Y</th>\n      <th>Number_Of_Dependant</th>\n      <th>Years_At_Residence</th>\n      <th>Years_At_Business</th>\n      <th>Nb_Of_Products</th>\n      <th>Net_Annual_Income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7440</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>573</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>12</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>18</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data_df = pd.read_csv('https://raw.githubusercontent.com/jyyang5/MAP569-project/master/CreditTraining.csv')\n",
    "label_df = data_df['Prod_Category'].tolist()\n",
    "\n",
    "# divide into categorical data and else as clean\n",
    "clean_df = data_df.select_dtypes(exclude=['object']).copy()\n",
    "cat_df = data_df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# replace comma with dot \n",
    "temp_list = []\n",
    "for ele in data_df['Net_Annual_Income'].tolist():\n",
    "  if type(ele) is not float:\n",
    "    temp_list.append(ele.replace(',','.'))\n",
    "  else:\n",
    "    temp_list.append(ele)\n",
    "\n",
    "clean_df['Net_Annual_Income'] = temp_list\n",
    "del cat_df['Net_Annual_Income']\n",
    "clean_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: there are categorical and timestamps that is not numeric.\n",
    "\n",
    "## 1.1. Deal with categorical data (timestamp data excluded)\n",
    "\n",
    "From all categorical datas ['Customer_Type', 'BirthDate', 'Customer_Open_Date', 'P_Client',\n",
    "       'Educational_Level', 'Marital_Status', 'Net_Annual_Income',\n",
    "       'Prod_Sub_Category', 'Prod_Decision_Date', 'Source',\n",
    "       'Type_Of_Residence', 'Prod_Closed_Date', 'Prod_Category']\n",
    "\n",
    "We first exclude ['BirthDate', 'Customer_Open_Date', 'Net_Annual_Income', 'Prod_Decision_Date']\n",
    "\n",
    "We use 0-1 encoding for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['Customer_Type', 'P_Client',\n",
    "            'Educational_Level', 'Marital_Status',\n",
    "            'Prod_Sub_Category', 'Source',\n",
    "            'Type_Of_Residence', 'Prod_Category']:\n",
    "        print(cat_df[name].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'Customer_Type', 'P_Client', 'Source' all have just two types we therefore restrict ourselves to binary variable.\n",
    "\n",
    "$$\n",
    "Customer\\_Type = \n",
    " \\begin{cases}\n",
    "1& \\text{Existing Client} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "P\\_Client = \n",
    " \\begin{cases}\n",
    "1& \\text{P Client} \\\\\n",
    "0 & \\text{NP Client}\n",
    "\\end{cases} \\\\\n",
    "Source= \n",
    " \\begin{cases}\n",
    "1& \\text{Sales} \\\\\n",
    "0 & \\text{Brranch}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add categorical data [timestamp data not added]\n",
    "clean_df1 = clean_df.join(pd.get_dummies(cat_df[['Customer_Type', 'P_Client',\n",
    "                                                'Educational_Level', 'Marital_Status',\n",
    "                                                'Prod_Sub_Category', 'Source',\n",
    "                                                'Type_Of_Residence', 'Prod_Category']]))\n",
    "\n",
    "clean_df1['Customer_Type'] = clean_df1['Customer_Type_Existing Client']\n",
    "del clean_df1['Customer_Type_Existing Client']\n",
    "del clean_df1['Customer_Type_Non Existing Client']\n",
    "\n",
    "clean_df1['P_Client'] = clean_df1['P_Client_NP_Client']\n",
    "del clean_df1['P_Client_NP_Client']\n",
    "del clean_df1['P_Client_P_Client']\n",
    "\n",
    "clean_df1['Source'] = clean_df1['Source_Branch']\n",
    "del clean_df1['Source_Branch']\n",
    "del clean_df1['Source_Sales']\n",
    "\n",
    "\n",
    "clean_df1[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Deal with timestamps \n",
    "**Appraoches**: We use duration so that the variable is comparable \n",
    "\n",
    "['BirthDate', 'Customer_Open_Date', 'Net_Annual_Income', 'Prod_Decision_Date', 'Prod_Closed_Date']\n",
    "\n",
    "\n",
    "- `Birth_Duration = Now - BirthDate` ('BirthDate'): assuming there is a distribution of credibility, just started working -> less credit, worked for a long time but not close to retirement -> high credit) \n",
    "- `Customer_Open_Duration = Now - Customer_Open_Date` ('Customer_Open_Date'): usually longer the history is, the more royal the customer is\n",
    "- 'Prod_Closed_Date' - 'Prod_Decision_Date'\n",
    "    - length of the product?: if product closed \n",
    "    - `Prod_not_closed = (Prod_closed != nan)`: dummy variable if the product is closed 'Prod_not_closed' = 0 if closed\n",
    "    - `Prod_Decision_Duration = Now - Prod_Decision_Date`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration: birth - now\n",
    "temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in cat_df['BirthDate'].tolist()]\n",
    "clean_df1['Birth_Duration'] = temp_list\n",
    "\n",
    "# duration: Customer_Open_Date\n",
    "temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in cat_df['Customer_Open_Date'].tolist()]\n",
    "clean_df1['Customer_Open_Duration'] = temp_list\n",
    "\n",
    "# dummy var: product closed = 1\n",
    "temp_list = [int(type(ele) != float) for ele in cat_df['Prod_Closed_Date'].tolist()]\n",
    "clean_df1['Prod_not_closed'] = temp_list\n",
    "\n",
    "# duration: Prod_Decision_Date\n",
    "temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in cat_df['Prod_Decision_Date'].tolist()]\n",
    "clean_df1['Prod_Decision_Duration'] = temp_list\n",
    "\n",
    "# convert to a list \n",
    "data_list = []\n",
    "labels = []\n",
    "for name in clean_df1.columns.values:\n",
    "    if name is 'Id_Customer':\n",
    "        pass\n",
    "    elif name is 'Y':\n",
    "        labels = clean_df1[name].tolist()\n",
    "    else:\n",
    "        data_list.append(clean_df1[name].tolist())\n",
    "\n",
    "data_list = np.transpose(data_list)\n",
    "data_list = np.array(data_list, dtype=np.float64)\n",
    "np.where(np.isnan(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to training and testing, we check if there is nan element\n",
    "\n",
    "Customers with nan \n",
    "- Years_At_Business\n",
    "    - Id_Customer = 398, 5882\t\n",
    "- Number_Of_Dependant\n",
    "    - Id_Customer = 8953, 9588\t\n",
    "- Net_Annual_Income\n",
    "    - Id_Customer = 9399, 9555\t\n",
    "\n",
    "## 1.3. Nan elemnts \n",
    "There are two usual approaches to deal with nan elements, the first one drops the data with nan, the second on the other hand replace the nan element. Since it is very likely that we may face future clients with unkown data (nan), so that we use the second approach to make the method proposed more general.\n",
    "\n",
    "- Median \n",
    "- Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df2 = clean_df1.copy()\n",
    "\n",
    "for name in ['Number_Of_Dependant', 'Years_At_Business', 'Net_Annual_Income']:\n",
    "    temp_list = np.array(clean_df2[name].tolist(), dtype=np.float64)\n",
    "    temp_list[np.isnan(temp_list)] = np.median(temp_list[~np.isnan(temp_list)])\n",
    "    # print(sum(np.isnan(temp_list)))\n",
    "    clean_df2[name] = temp_list\n",
    "\n",
    "# convert to train mat \n",
    "data_list = []\n",
    "labels = []\n",
    "for name in clean_df2.columns.values:\n",
    "    if name == 'Id_Customer':\n",
    "        pass\n",
    "        \n",
    "    elif name is 'Y':\n",
    "        labels = clean_df2[name].tolist()\n",
    "    else:\n",
    "        data_list.append(clean_df2[name].tolist())\n",
    "\n",
    "data_list = np.transpose(data_list)\n",
    "data_list = np.array(data_list, dtype=np.float64)\n",
    "np.where(np.isnan(data_list))\n",
    "train_df = clean_df2.copy()\n",
    "del train_df['Y']\n",
    "del train_df['Id_Customer']\n",
    "'Check done: ', np.shape(train_df)== np.shape(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_list, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# np.save('X_train.npy',X_train)\n",
    "# np.save('X_test.npy',X_test)\n",
    "# np.save('y_train.npy',y_train)\n",
    "# np.save('y_test.npy',y_test)\n",
    "\n",
    "print('-----------all-----------')\n",
    "for i in set(labels):\n",
    "    print(i,labels.count(i))\n",
    "print('percentage of 1:', labels.count(1)/len(labels))\n",
    "\n",
    "print('-----------train-----------')\n",
    "for i in set(labels):\n",
    "    print(i,y_train.count(i))\n",
    "print('percentage of 1:', y_train.count(1)/len(y_train))\n",
    "\n",
    "print('-----------test-----------')\n",
    "for i in set(labels):\n",
    "    print(i,y_test.count(i))\n",
    "print('percentage of 1:', y_test.count(1)/len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to be unbalanced (two labels has a relative big difference in percentag), we first try without balancing the data.\n",
    "\n",
    "## 2.1. Evaluation metrics\n",
    "\n",
    "| \t\t|     same clusters|   different clusters|\n",
    "| :-------- | --------:| :------: |\n",
    "| same class    |   TP  |  FN |\n",
    "|different class|   FP  |  TN |\n",
    "- $P(precision) = \\frac{TP}{TP+FP}$ \n",
    "- $R(recall) = \\frac{TP}{TP+FN}$ \n",
    "- $F_{\\beta}= \\frac{(\\beta^2+1)PR}{\\beta^2P+R}$\n",
    "\n",
    "### Note: since label=1 means that the client has defaulted on its credit, which is something that we definately want to avoid, we focus on *R(recall)* (percentage of detected clients among all truly defaulted clients) \n",
    "\n",
    "\n",
    "## 2.2. Models\n",
    "Here we use two types of models \n",
    "- **Intrinsic**: if after feature engineering, the data itself could seperate.\n",
    "    - KNN\n",
    "    - Kmeans\n",
    "    - SVM\n",
    "- **Instrumental**: might contain some sematic meaning (relationshop), that is not explict so that we need more complex models. \n",
    "    - Random forest \n",
    "    - XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lables(labels):\n",
    "    \"\"\" convert the input (0/1) labels to what is needed \n",
    "    return a binary label sequence \n",
    "    \"\"\"\n",
    "    labels = np.array(labels)\n",
    "    if sum(labels) > len(labels)/2:\n",
    "        labels[labels==1] = -1\n",
    "        labels[labels==0] = 1\n",
    "        labels[labels==-1] = 0\n",
    "    return labels\n",
    "\n",
    "def eval_metric(y_test, preds):\n",
    "    \"\"\" \n",
    "    print the classification report \n",
    "    \"\"\"\n",
    "    \n",
    "    preds = convert_lables(preds)\n",
    "    print('confusion matrix')\n",
    "    print(confusion_matrix(y_test, preds))\n",
    "    print('\\n')\n",
    "    print('summary')\n",
    "    print(classification_report(y_test, preds))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds, pos_label=2)\n",
    "    metrics.auc(fpr, tpr)\n",
    "\n",
    "def test_deterministic(X_train, y_train):\n",
    "    \"\"\" print the evaluation results of deterministic models\n",
    "    \"\"\"\n",
    "    # KNeighbors\n",
    "    neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    pred_KNN = neigh.predict(X_test)\n",
    "    print('----------------KNeighbors-------------------')\n",
    "    eval_metric(y_test, pred_KNN)\n",
    "\n",
    "    # Kmeans\n",
    "    neigh = KMeans(n_clusters=2)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    pred_Kmeans = convert_lables(neigh.predict(X_test))\n",
    "    print('----------------Kmeans-------------------')\n",
    "    eval_metric(y_test, pred_Kmeans)\n",
    "\n",
    "    # SVM\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_SVM = convert_lables(clf.predict(X_test))\n",
    "    print('----------------SVM-------------------')\n",
    "    eval_metric(y_test, pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_non_deterministic(X_train, y_train):\n",
    "    \"\"\" print the evaluation results of non-deterministic models\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # random forest model \n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train,y_train)\n",
    "    # predictions\n",
    "    pred_rfc = rfc.predict(X_test)\n",
    "    print('----------------randomForest-------------------')\n",
    "    eval_metric(pred_rfc, y_test)\n",
    "\n",
    "    rfc_importances = rfc.feature_importances_\n",
    "    rfc_std = np.std([rfc.feature_importances_ for tree in rfc.estimators_], axis=0)\n",
    "    rfc_indices = np.argsort(rfc_importances)[::-1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        print(\"%d. feature %s (%f)\" % (f + 1, train_df.columns.tolist()[rfc_indices[f]], rfc_importances[rfc_indices[f]]))\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X_train.shape[1]), rfc_importances[rfc_indices],\n",
    "        color=\"r\", yerr=rfc_std[rfc_indices], align=\"center\")\n",
    "    plt.xticks(range(X_train.shape[1]), rfc_indices)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "    ################################\n",
    "    # XGBoost\n",
    "    xg_reg = xgb.XGBRegressor(objective ='reg:logistic') \n",
    "    xg_reg.fit(X_train, y_train)\n",
    "    xg_preds = (xg_reg.predict(X_test)>=0.5)*1\n",
    "    print('----------------XGBoost-------------------')\n",
    "    eval_metric(xg_preds, y_test)\n",
    "    # plot feature importance\n",
    "    xg_importances = xg_reg.feature_importances_\n",
    "    xg_indices = np.argsort(xg_importances)[::-1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(X_train.shape[1]):\n",
    "        print(\"%d. feature %s (%f)\" % (f + 1, train_df.columns.tolist()[xg_indices[f]], xg_importances[xg_indices[f]]))\n",
    "\n",
    "    plot_importance(xg_reg)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_deterministic(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_non_deterministic(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- indeed, the recall reaches 0.76 and 0.63 for RF and XGboost even if the dataset is unbalanced\n",
    "- from the feature importance, we can infer that the feature reconstructed is indeed helpful ('Prod_not_closed' is the most dominating feature in both models)\n",
    "\n",
    "## 2.3. Balance data\n",
    "In Section 2.2, we did some experiments but find that even if the overall accurancy is high, the prediction results of clients with label=1 is not good. Especially with classifiers like SVM, the class with more samples (majority class) is favoured. We therefore resample the minority class to balance the train data and see how the result might improve. Since the size of the minority class is very small, we w.l.o.g. use Upsampling.   \n",
    "\n",
    "# 3. Encapsulation \n",
    "We combine all mentioned, add a few more functionalities and create the class below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Credit_predictor():\n",
    "    def __init__(self,path):\n",
    "        \"\"\"\n",
    "        read the data\n",
    "        \"\"\"\n",
    "        self.raw_data = pd.read_csv('CreditTraining.csv')\n",
    "        self.clean_df = self.raw_data.select_dtypes(exclude=['object']).copy()\n",
    "        self.cat_df = self.raw_data.select_dtypes(include=['object']).copy()\n",
    "        # categorical data \n",
    "        self.cates = ['Customer_Type', 'P_Client',\n",
    "            'Educational_Level', 'Marital_Status',\n",
    "            'Prod_Sub_Category', 'Source',\n",
    "            'Type_Of_Residence', 'Prod_Category']\n",
    "        # all timestamp data \n",
    "        self.date_trans_set = {'Birth_Duration':'BirthDate',\n",
    "             'Customer_Open_Duration':'Customer_Open_Date',\n",
    "             'Prod_Decision_Duration':'Prod_Decision_Date'}\n",
    "        \n",
    "    def data_preprocessing_0(self):\n",
    "        \"\"\"\n",
    "        seperate the data into numerical set and categorical set\n",
    "        \"\"\"\n",
    "        self.clean_df = self.raw_data.select_dtypes(exclude=['object']).copy()\n",
    "        self.cat_df = self.raw_data.select_dtypes(include=['object']).copy()\n",
    "        temp_list = []\n",
    "        # convert value from comma to dot \n",
    "        for ele in self.raw_data['Net_Annual_Income'].tolist():\n",
    "            if type(ele) is not float:\n",
    "                temp_list.append(ele.replace(',','.'))\n",
    "            else:\n",
    "                temp_list.append(ele)\n",
    "        self.clean_df['Net_Annual_Income'] = temp_list\n",
    "        del self.cat_df['Net_Annual_Income']\n",
    "        self.dates_transformer()\n",
    "        \n",
    "    def dates_transformer(self):\n",
    "        \"\"\"\n",
    "        processing for the date\n",
    "        \"\"\"\n",
    "        # convert timestamp to duration: now - timestamp \n",
    "        for i in self.date_trans_set:\n",
    "            temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in self.cat_df[self.date_trans_set[i]].tolist()]\n",
    "            self.clean_df[i] = temp_list\n",
    "            del self.cat_df[self.date_trans_set[i]]\n",
    "        # indicator whether product is closed \n",
    "        temp_list = [int(type(ele) != float) for ele in self.cat_df['Prod_Closed_Date'].tolist()]\n",
    "        self.clean_df['Prod_not_closed'] = temp_list\n",
    "        del self.cat_df['Prod_Closed_Date']\n",
    "\n",
    "    def data_preprocessing_simple(self):\n",
    "        self.data_simple = self.clean_df.copy()\n",
    "        for i in self.cates:\n",
    "            labelencoder = LabelEncoder()\n",
    "            labelencoder.fit(self.cat_df[i])\n",
    "            self.data_simple[i] = labelencoder.transform(self.cat_df[i])\n",
    "        \n",
    "    def data_preprocessing_onehot(self):\n",
    "        binary = ['Customer_Type', 'P_Client', 'Source']\n",
    "        no_binary = [i for i in self.cates if i not in binary]\n",
    "        self.data_oh = self.clean_df.copy()\n",
    "        for i in binary:\n",
    "            labelencoder = LabelEncoder()\n",
    "            labelencoder.fit(self.cat_df[i])\n",
    "            \"\"\"\n",
    "            To see the representation of the labels : list(labelencoder.classes_)\n",
    "            \"\"\"\n",
    "            self.data_oh[i] = labelencoder.transform(self.cat_df[i])\n",
    "        self.data_oh = self.data_oh.join(pd.get_dummies(self.cat_df[no_binary]))\n",
    "        \n",
    "    def split_dataset(self,dataset):\n",
    "        X = dataset.drop('Y',axis = 1)\n",
    "        Y = dataset.Y\n",
    "        try:\n",
    "            imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "            imp_mean.fit(X)\n",
    "        except:\n",
    "            imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "            imp_mean.fit(X)\n",
    "        '''\n",
    "        SimpleImputer:\n",
    "        strategy : mean, median, most_frequent, constant\n",
    "        '''\n",
    "        X_imputed = imp_mean.transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_imputed, Y, test_size=0.25, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    def split_dataset_4_imbal(self,dataset):\n",
    "        X_train, X_test, y_train, y_test = self.split_dataset(dataset)\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "   #     ros = RandomOverSampler(random_state=0)\n",
    "   #     X_test, y_test = ros.fit_resample(X_test, y_test)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    \n",
    "    def convert_labels(self,labels):\n",
    "        \"\"\" \n",
    "        convert the input (0/1) labels to what is needed \n",
    "        return a binary label sequence \n",
    "        \"\"\"\n",
    "        labels = np.array(labels)\n",
    "        if sum(labels) > len(labels)/2:\n",
    "            labels[labels==1] = -1\n",
    "            labels[labels==0] = 1\n",
    "            labels[labels==-1] = 0\n",
    "        return labels\n",
    "\n",
    "   \n",
    "    def eval_metric(self, y_test, preds):\n",
    "        \"\"\" \n",
    "        print the classification report \n",
    "        \"\"\"\n",
    "     #   preds = convert_labels(preds)\n",
    "        print('confusion matrix')\n",
    "        print(confusion_matrix(y_test, preds))\n",
    "        print('\\n')\n",
    "        print('summary')\n",
    "        print(classification_report(y_test, preds))\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, preds, pos_label=2)\n",
    "        metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'CreditTraining.csv'\n",
    "Cp = Credit_predictor(path)\n",
    "Cp.data_preprocessing_0()\n",
    "Cp.data_preprocessing_simple()\n",
    "Cp.data_preprocessing_onehot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot\n",
    "X_train, X_test, y_train, y_test = Cp.split_dataset(Cp.data_oh)\n",
    "# one-hot [balanced]\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = Cp.split_dataset_4_imbal(Cp.data_oh)\n",
    "# categorical -> 0:n_classes-1\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = Cp.split_dataset(Cp.data_simple)\n",
    "# categorical -> 0:n_classes-1 [balanced]\n",
    "X_train_tb, X_test_tb, y_train_tb, y_test_tb = Cp.split_dataset_4_imbal(Cp.data_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, Cp):\n",
    "    \"\"\"\n",
    "    print the evaluation result of the model with the Cp object \n",
    "    \"\"\"\n",
    "\n",
    "    print('----------------one-hot-------------------')\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_y = model.predict(X_test)\n",
    "    Cp.eval_metric(y_test, pred_y)\n",
    "    print('----------------one-hot[balanced]-------------------')\n",
    "    model.fit(X_train_b, y_train_b)\n",
    "    pred_y_b = model.predict(X_test_b)\n",
    "    Cp.eval_metric(y_test_b, pred_y_b)\n",
    "    print('----------------0:n_classes-1-------------------')\n",
    "    model.fit(X_train_t, y_train_t)\n",
    "    pred_y_t = model.predict(X_test_t)\n",
    "    Cp.eval_metric(y_test_t, pred_y_t)\n",
    "    print('----------------0:n_classes-1[balanced]-------------------')\n",
    "    model.fit(X_train_tb, y_train_tb)\n",
    "    pred_y_tb = model.predict(X_test_tb)\n",
    "    Cp.eval_metric(y_test_tb, pred_y_tb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Model selection \n",
    "### 3.1.1 LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "model_eval(lr, Cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=100)\n",
    "model_eval(rfc, Cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "pred_SVM = clf.predict(X_test)\n",
    "\n",
    "Cp.eval_metric(y_test, pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n = normalize(X_train_b, norm='l2')\n",
    "X_test_n = normalize(X_test_b, norm='l2')\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train_n, y_train_b)\n",
    "pred_SVM_b = clf.predict(X_test_n)\n",
    "\n",
    "Cp.eval_metric(y_test_b, pred_SVM_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "model_eval(LDA, Cp)LDA = LinearDiscriminantAnalysis()\n",
    "model_eval(LDA, Cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5. XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBClassifier(objective ='reg:logistic')\n",
    "model_eval(xg_reg, Cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_tweedie = xgb.XGBClassifier(objective ='binary:logitraw', eval_metric = 'error@0.5')\n",
    "model_eval(xg_tweedie, Cp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing objective and evaluation metric do not seem to have a huge difference.\n",
    "\n",
    "**Observations**\n",
    "- Balancing the data indeed helps to improve the model performance \n",
    "- We **select logistiRegression, LDA, and XGBoost given their $R_{\\text{label=1}}$ around 0.85, $P_{\\text{label=0}}=0.99$, $P_{\\text{label=1}}$ around 0.42 using the two balanced data.**\n",
    "\n",
    "\n",
    "## 3.2. Parameter Tuning \n",
    "\n",
    "In this section we perform more fine-grined parametr tuning for the three models mentioned.\n",
    "\n",
    "$\\color{red}{\\text{TODO:}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Voting  \n",
    "\n",
    "In this section, we propose two voting methods that utilize the predictions of several models and hopes it would maximize the information obatined so far.\n",
    "- Improve precision of predicted defaulted\n",
    "- Improve coverage of truly defaulted\n",
    "\n",
    "## 4.1. Minimize innocent (Improve precision of predicted defaulted)\n",
    "\n",
    "Since we have three models **logistiRegression, LDA, and XGBoost** using balanced data have $R_{\\text{label=1}}$ around 0.85, meaning most of the defaulted customers are detected (which we think is a **relative good coverage**).\n",
    "\n",
    "But $\\text{label=1} \\approx 0.42$ , meaning more than half of the predicted is not defaulted, and it would be costly to do a thorough investigation of the users so we want to **minimize those innocent but detected**.   \n",
    "\n",
    "With 3 predictions from 3 individual parties, we want to extract the mutual information and believing that if one client is predictd defaulted by all parties then there is a larger probability that the one is defaulted compared with (1/3 of the parties voted so). And of course, we might lose some covergae. **We try taking a logical AND to do this**.\n",
    "\n",
    "**Essentially, we are doing a voting by setting the weight to be 100% iff. all parties vote 1, ensuring that the number of inocent client is minimized**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_tb, y_train_tb)\n",
    "lr_pred_y_tb = lr.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y_tb)\n",
    "\n",
    "xg_reg.fit(X_train_tb, y_train_tb)\n",
    "xg_pred_y_tb = xg_reg.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, xg_pred_y_tb)\n",
    "\n",
    "LDA.fit(X_train_tb, y_train_tb)\n",
    "LDA_pred_y_tb = LDA.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, LDA_pred_y_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cp.eval_metric(y_test_tb, LDA_pred_y_tb & xg_pred_y_tb & lr_pred_y_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: we did increase $P_{\\text{label=1}}$ to 0.44, but $R_{\\text{label=1}}$ drops to 0.84, the improvement of $P_{\\text{label=1}}$ is small, one potential explaination is that the three parties share a lot of mutual information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "parties_pred_mat = [LDA_pred_y_tb, xg_pred_y_tb, lr_pred_y_tb]\n",
    "distance.cdist(parties_pred_mat, parties_pred_mat, 'hamming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the *Minimize innocent* voting mechanism fail to improve a $P_{\\text{label=1}}$ a lot is largely due to the percentage of shared information is dominant for all three parties (around 1% difference).\n",
    "\n",
    "## 4.2. Add trustworthy information ( Improve coverage of truly defaulted)\n",
    "\n",
    "We use one of the three parties as a base (with good covergae) and then we add prediction by party with high $P_{\\text{label=1}}$ but possibly low coverage ($R_{\\text{label=1}}$), so as to improve precison and recall at the same time.\n",
    "\n",
    "**Essentially, we are doing a voting by setting the weight of trustworthy party to 100%, meaning once they vote 1, the client is 1 in prediction.**\n",
    "\n",
    "- Base\n",
    "    - `lr_pred_y_tb`: $P_{\\text{label=1}}=0.43, R_{\\text{label=1}}=0.88$ \n",
    "    - `xg_pred_y_tb`: $P_{\\text{label=1}}=0.43, R_{\\text{label=1}}=0.85$ \n",
    "- Trustworthy\n",
    "    - `lr_pred_y` : $P_{\\text{label=1}}=0.80, R_{\\text{label=1}}=0.15$ \n",
    "    - `rfc_pred_y` : $P_{\\text{label=1}}=0.76, R_{\\text{label=1}}=0.28$       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred_y = rfc.predict(X_test)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred_y = lr.predict(X_test)\n",
    "\n",
    "\n",
    "print('-------------------LDA+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | LDA_pred_y_tb)\n",
    "\n",
    "print('-------------------LDA+rfc-------------------')\n",
    "Cp.eval_metric(y_test_tb, rfc_pred_y | LDA_pred_y_tb)\n",
    "\n",
    "print('-------------------LDA+rfc+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | rfc_pred_y | LDA_pred_y_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------XG+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | xg_pred_y_tb)\n",
    "\n",
    "print('-------------------XG+rfc-------------------')\n",
    "Cp.eval_metric(y_test_tb, rfc_pred_y | xg_pred_y_tb)\n",
    "\n",
    "print('-------------------XG+rfc+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | rfc_pred_y | xg_pred_y_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, these *trustworthy information* seem to have been covered already by the parties with good coverage.\n",
    "\n",
    "# 5. Conclusion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}