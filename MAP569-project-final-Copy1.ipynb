{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn import metrics\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "# import xgboost as xgb\n",
    "# from xgboost import plot_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('https://raw.githubusercontent.com/jyyang5/MAP569-project/master/CreditTraining.csv')\n",
    "label_df = data_df['Prod_Category'].tolist()\n",
    "\n",
    "# divide into categorical data and else as clean\n",
    "clean_df = data_df.select_dtypes(exclude=['object']).copy()\n",
    "cat_df = data_df.select_dtypes(include=['object']).copy()\n",
    "\n",
    "# replace comma with dot \n",
    "temp_list = []\n",
    "for ele in data_df['Net_Annual_Income'].tolist():\n",
    "  if type(ele) is not float:\n",
    "    temp_list.append(ele.replace(',','.'))\n",
    "  else:\n",
    "    temp_list.append(ele)\n",
    "\n",
    "clean_df['Net_Annual_Income'] = temp_list\n",
    "del cat_df['Net_Annual_Income']\n",
    "clean_df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: there are categorical and timestamps that is not numeric.\n",
    "\n",
    "## 1.1. Deal with categorical data (timestamp data excluded)\n",
    "\n",
    "From all categorical datas ['Customer_Type', 'BirthDate', 'Customer_Open_Date', 'P_Client',\n",
    "       'Educational_Level', 'Marital_Status', 'Net_Annual_Income',\n",
    "       'Prod_Sub_Category', 'Prod_Decision_Date', 'Source',\n",
    "       'Type_Of_Residence', 'Prod_Closed_Date', 'Prod_Category']\n",
    "\n",
    "We first exclude ['BirthDate', 'Customer_Open_Date', 'Net_Annual_Income', 'Prod_Decision_Date']\n",
    "\n",
    "We use 0-1 encoding for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['Customer_Type', 'P_Client',\n",
    "            'Educational_Level', 'Marital_Status',\n",
    "            'Prod_Sub_Category', 'Source',\n",
    "            'Type_Of_Residence', 'Prod_Category']:\n",
    "        print(cat_df[name].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'Customer_Type', 'P_Client', 'Source' all have just two types we therefore restrict ourselves to binary variable.\n",
    "\n",
    "$$\n",
    "Customer\\_Type = \n",
    " \\begin{cases}\n",
    "1& \\text{Existing Client} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases} \\\\\n",
    "P\\_Client = \n",
    " \\begin{cases}\n",
    "1& \\text{P Client} \\\\\n",
    "0 & \\text{NP Client}\n",
    "\\end{cases} \\\\\n",
    "Source= \n",
    " \\begin{cases}\n",
    "1& \\text{Sales} \\\\\n",
    "0 & \\text{Brranch}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add categorical data [timestamp data not added]\n",
    "clean_df1 = clean_df.join(pd.get_dummies(cat_df[['Customer_Type', 'P_Client',\n",
    "                                                'Educational_Level', 'Marital_Status',\n",
    "                                                'Prod_Sub_Category', 'Source',\n",
    "                                                'Type_Of_Residence', 'Prod_Category']]))\n",
    "\n",
    "clean_df1['Customer_Type'] = clean_df1['Customer_Type_Existing Client']\n",
    "del clean_df1['Customer_Type_Existing Client']\n",
    "del clean_df1['Customer_Type_Non Existing Client']\n",
    "\n",
    "clean_df1['P_Client'] = clean_df1['P_Client_NP_Client']\n",
    "del clean_df1['P_Client_NP_Client']\n",
    "del clean_df1['P_Client_P_Client']\n",
    "\n",
    "clean_df1['Source'] = clean_df1['Source_Branch']\n",
    "del clean_df1['Source_Branch']\n",
    "del clean_df1['Source_Sales']\n",
    "\n",
    "\n",
    "clean_df1[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Deal with timestamps \n",
    "**Appraoches**: We use duration so that the variable is comparable \n",
    "\n",
    "['BirthDate', 'Customer_Open_Date', 'Net_Annual_Income', 'Prod_Decision_Date', 'Prod_Closed_Date']\n",
    "\n",
    "\n",
    "- `Birth_Duration = Now - BirthDate` ('BirthDate'): assuming there is a distribution of credibility, just started working -> less credit, worked for a long time but not close to retirement -> high credit) \n",
    "- `Customer_Open_Duration = Now - Customer_Open_Date` ('Customer_Open_Date'): usually longer the history is, the more royal the customer is\n",
    "- 'Prod_Closed_Date' - 'Prod_Decision_Date'\n",
    "    - length of the product?: if product closed \n",
    "    - `Prod_not_closed = (Prod_closed != nan)`: dummy variable if the product is closed 'Prod_not_closed' = 0 if closed\n",
    "    - `Prod_Decision_Duration = Now - Prod_Decision_Date`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration: birth - now\n",
    "temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in cat_df['BirthDate'].tolist()]\n",
    "clean_df1['Birth_Duration'] = temp_list\n",
    "\n",
    "# duration: Customer_Open_Date\n",
    "temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in cat_df['Customer_Open_Date'].tolist()]\n",
    "clean_df1['Customer_Open_Duration'] = temp_list\n",
    "\n",
    "# dummy var: product closed = 1\n",
    "temp_list = [int(type(ele) != float) for ele in cat_df['Prod_Closed_Date'].tolist()]\n",
    "clean_df1['Prod_not_closed'] = temp_list\n",
    "\n",
    "# duration: Prod_Decision_Date\n",
    "temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in cat_df['Prod_Decision_Date'].tolist()]\n",
    "clean_df1['Prod_Decision_Duration'] = temp_list\n",
    "\n",
    "# convert to a list \n",
    "data_list = []\n",
    "labels = []\n",
    "for name in clean_df1.columns.values:\n",
    "    if name is 'Id_Customer':\n",
    "        pass\n",
    "    elif name is 'Y':\n",
    "        labels = clean_df1[name].tolist()\n",
    "    else:\n",
    "        data_list.append(clean_df1[name].tolist())\n",
    "\n",
    "data_list = np.transpose(data_list)\n",
    "data_list = np.array(data_list, dtype=np.float64)\n",
    "np.where(np.isnan(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to training and testing, we check if there is nan element\n",
    "\n",
    "Customers with nan \n",
    "- Years_At_Business\n",
    "    - Id_Customer = 398, 5882\t\n",
    "- Number_Of_Dependant\n",
    "    - Id_Customer = 8953, 9588\t\n",
    "- Net_Annual_Income\n",
    "    - Id_Customer = 9399, 9555\t\n",
    "\n",
    "## 1.3. Nan elements \n",
    "There are two usual approaches to deal with nan elements, the first one drops the data with nan, the second on the other hand replace the nan element. Since it is very likely that we may face future clients with unkown data (nan), so that we use the second approach to make the method proposed more general.\n",
    "\n",
    "- Median \n",
    "- Most frequent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df2 = clean_df1.copy()\n",
    "\n",
    "for name in ['Number_Of_Dependant', 'Years_At_Business', 'Net_Annual_Income']:\n",
    "    temp_list = np.array(clean_df2[name].tolist(), dtype=np.float64)\n",
    "    temp_list[np.isnan(temp_list)] = np.median(temp_list[~np.isnan(temp_list)])\n",
    "    # print(sum(np.isnan(temp_list)))\n",
    "    clean_df2[name] = temp_list\n",
    "\n",
    "# convert to train mat \n",
    "data_list = []\n",
    "labels = []\n",
    "for name in clean_df2.columns.values:\n",
    "    if name == 'Id_Customer':\n",
    "        pass\n",
    "        \n",
    "    elif name is 'Y':\n",
    "        labels = clean_df2[name].tolist()\n",
    "    else:\n",
    "        data_list.append(clean_df2[name].tolist())\n",
    "\n",
    "data_list = np.transpose(data_list)\n",
    "data_list = np.array(data_list, dtype=np.float64)\n",
    "np.where(np.isnan(data_list))\n",
    "train_df = clean_df2.copy()\n",
    "del train_df['Y']\n",
    "del train_df['Id_Customer']\n",
    "'Check done: ', np.shape(train_df)== np.shape(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_list, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# np.save('X_train.npy',X_train)\n",
    "# np.save('X_test.npy',X_test)\n",
    "# np.save('y_train.npy',y_train)\n",
    "# np.save('y_test.npy',y_test)\n",
    "\n",
    "print('-----------all-----------')\n",
    "for i in set(labels):\n",
    "    print(i,labels.count(i))\n",
    "print('percentage of 1:', labels.count(1)/len(labels))\n",
    "\n",
    "print('-----------train-----------')\n",
    "for i in set(labels):\n",
    "    print(i,y_train.count(i))\n",
    "print('percentage of 1:', y_train.count(1)/len(y_train))\n",
    "\n",
    "print('-----------test-----------')\n",
    "for i in set(labels):\n",
    "    print(i,y_test.count(i))\n",
    "print('percentage of 1:', y_test.count(1)/len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to be unbalanced (two labels has a relative big difference in percentag), we first try without balancing the data.\n",
    "\n",
    "## 2.1. Evaluation metrics\n",
    "\n",
    "| \t\t|     same clusters|   different clusters|\n",
    "| :-------- | --------:| :------: |\n",
    "| same class    |   TP  |  FN |\n",
    "|different class|   FP  |  TN |\n",
    "- $P(precision) = \\frac{TP}{TP+FP}$ \n",
    "- $R(recall) = \\frac{TP}{TP+FN}$ \n",
    "- $F_{\\beta}= \\frac{(\\beta^2+1)PR}{\\beta^2P+R}$\n",
    "\n",
    "### Note: since label=1 means that the client has defaulted on its credit, which is something that we definately want to avoid, we focus on *R(recall)* (percentage of detected clients among all truly defaulted clients) . In order to taken into consideration the *P(precision)* (the percentage of true clients among all detected clients), we use the *F 0.5-score*, the * $F_\\beta$-score where $\\beta = 2$, as the evaluation of the model\n",
    "\n",
    "\n",
    "## 2.2. Primal Test with Basic Model: SVM, KNN & Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_metric(y_test, preds):\n",
    "    \"\"\" \n",
    "    print the classification report \n",
    "    \"\"\"\n",
    "    print('confusion matrix')\n",
    "    print(confusion_matrix(y_test, preds))\n",
    "    print('\\n')\n",
    "    print('summary')\n",
    "    beta = 2\n",
    "#    names = [\"precision\", \"recall\" , \"f{}-score\".format(beta) ,\"support\"]\n",
    "#    print(\"       precision    recall  f{}-score   support\".format(beta))\n",
    "    res = precision_recall_fscore_support(y_test, preds, beta = beta, pos_label = 1, average = 'binary')\n",
    "    print(\"precision:{}, recall:{}\\n f{}-score:{}, support:{}\".format(round(res[0],2),round(res[1],2),beta,round(res[2],2),res[3],2))\n",
    " #   print(classification_report(y_test, preds))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds, pos_label=2)\n",
    "    metrics.auc(fpr, tpr)\n",
    "\n",
    "def tests(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" print the evaluation results of deterministic models\n",
    "    \"\"\"\n",
    "    # KNeighbors\n",
    "    neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    pred_KNN = neigh.predict(X_test)\n",
    "    print('----------------K-Neighbors-------------------')\n",
    "    eval_metric(y_test, pred_KNN)\n",
    "\n",
    "    # SVM\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred_SVM = clf.predict(X_test)\n",
    "    print('----------------SVM-------------------')\n",
    "    eval_metric(y_test, pred_SVM)\n",
    "    \n",
    "    #Decision Tree\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    dt.fit(X_train, y_train)\n",
    "    pred_DT = dt.predict(X_test)\n",
    "    print('-----------------Decision Tree------------------')\n",
    "    eval_metric(y_test, pred_DT)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e1acab764f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "tests(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- The accuracy of the three basic models reaches 0.9, which is satisfaisant for other ML tasks. However, we can notice that the SVM and KNN models predict that all the labels are '0', thanks to the imbalance of the data, the accuracy is still high with this sort of prediction, but it is not what we want. This is one reason why we chose to focus on Recall, Precision and F-score instead of Accuracy.\n",
    "- The rebalancing of input data is necessary espacially for models such as SVM, KNN and LR \n",
    "- The F0.5-score of Decision Tree reaches 0.48 with imbalanced data, this shows that the Decision Tree, as well as random forest, which will be used in the next part, could work well on imbalanced data.\n",
    "\n",
    "## 2.3. Balance data\n",
    "In Section 2.2, we did some experiments but find that even if the overall accurancy is high, the prediction results of clients with label=1 is not good. Especially with classifiers like SVM, the class with more samples (majority class) is favoured. We therefore resample the minority class to balance the train data and see how the result might improve. Since the size of the minority class is very small, we w.l.o.g. use Upsampling.   \n",
    "\n",
    "# 3. Encapsulation \n",
    "We combine all mentioned, add a few more functionalities and create the class below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Credit_predictor():\n",
    "    def __init__(self,path):\n",
    "        \"\"\"\n",
    "        read the data\n",
    "        \"\"\"\n",
    "        self.raw_data = pd.read_csv(path)\n",
    "        self.clean_df = self.raw_data.select_dtypes(exclude=['object']).copy()\n",
    "        self.cat_df = self.raw_data.select_dtypes(include=['object']).copy()\n",
    "        # categorical data \n",
    "        self.cates = ['Customer_Type', 'P_Client',\n",
    "            'Educational_Level', 'Marital_Status',\n",
    "            'Prod_Sub_Category', 'Source',\n",
    "            'Type_Of_Residence', 'Prod_Category']\n",
    "        # all timestamp data \n",
    "        self.date_trans_set = {'Birth_Duration':'BirthDate',\n",
    "             'Customer_Open_Duration':'Customer_Open_Date',\n",
    "             'Prod_Decision_Duration':'Prod_Decision_Date'}\n",
    "        \n",
    "    def data_preprocessing_0(self):\n",
    "        \"\"\"\n",
    "        seperate the data into numerical set and categorical set\n",
    "        \"\"\"\n",
    "        self.clean_df = self.raw_data.select_dtypes(exclude=['object']).copy()\n",
    "        self.cat_df = self.raw_data.select_dtypes(include=['object']).copy()\n",
    "        temp_list = []\n",
    "        # convert value from comma to dot \n",
    "        for ele in self.raw_data['Net_Annual_Income'].tolist():\n",
    "            if type(ele) is not float:\n",
    "                temp_list.append(ele.replace(',','.'))\n",
    "            else:\n",
    "                temp_list.append(ele)\n",
    "        self.clean_df['Net_Annual_Income'] = temp_list\n",
    "        del self.cat_df['Net_Annual_Income']\n",
    "        self.dates_transformer()\n",
    "        \n",
    "    def dates_transformer(self):\n",
    "        \"\"\"\n",
    "        processing for the date\n",
    "        \"\"\"\n",
    "        # convert timestamp to duration: now - timestamp \n",
    "        for i in self.date_trans_set:\n",
    "            temp_list = [(datetime.now().date() - datetime.strptime(datetime_str, '%d/%m/%Y').date()).days for datetime_str in self.cat_df[self.date_trans_set[i]].tolist()]\n",
    "            self.clean_df[i] = temp_list\n",
    "            del self.cat_df[self.date_trans_set[i]]\n",
    "        # indicator whether product is closed \n",
    "        temp_list = [int(type(ele) != float) for ele in self.cat_df['Prod_Closed_Date'].tolist()]\n",
    "        self.clean_df['Prod_not_closed'] = temp_list\n",
    "        del self.cat_df['Prod_Closed_Date']\n",
    "\n",
    "    def data_preprocessing_simple(self):\n",
    "        self.data_simple = self.clean_df.copy()\n",
    "        for i in self.cates:\n",
    "            labelencoder = LabelEncoder()\n",
    "            labelencoder.fit(self.cat_df[i])\n",
    "            self.data_simple[i] = labelencoder.transform(self.cat_df[i])\n",
    "        \n",
    "    def data_preprocessing_onehot(self):\n",
    "        binary = ['Customer_Type', 'P_Client', 'Source']\n",
    "        no_binary = [i for i in self.cates if i not in binary]\n",
    "        self.data_oh = self.clean_df.copy()\n",
    "        for i in binary:\n",
    "            labelencoder = LabelEncoder()\n",
    "            labelencoder.fit(self.cat_df[i])\n",
    "            \"\"\"\n",
    "            To see the representation of the labels : list(labelencoder.classes_)\n",
    "            \"\"\"\n",
    "            self.data_oh[i] = labelencoder.transform(self.cat_df[i])\n",
    "        self.data_oh = self.data_oh.join(pd.get_dummies(self.cat_df[no_binary]))\n",
    "        \n",
    "    def split_dataset(self,dataset):\n",
    "        X = dataset.drop('Y',axis = 1)\n",
    "        Y = dataset.Y\n",
    "        try:\n",
    "            imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "            imp_mean.fit(X)\n",
    "        except:\n",
    "            imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "            imp_mean.fit(X)\n",
    "        '''\n",
    "        SimpleImputer:\n",
    "        strategy : mean, median, most_frequent, constant\n",
    "        '''\n",
    "        X_imputed = imp_mean.transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_imputed, Y, test_size=0.25, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    def split_dataset_4_imbal(self,dataset):\n",
    "        X_train, X_test, y_train, y_test = self.split_dataset(dataset)\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "   #     ros = RandomOverSampler(random_state=0)\n",
    "   #     X_test, y_test = ros.fit_resample(X_test, y_test)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    \n",
    "    def convert_labels(self,labels):\n",
    "        \"\"\" \n",
    "        convert the input (0/1) labels to what is needed \n",
    "        return a binary label sequence \n",
    "        \"\"\"\n",
    "        labels = np.array(labels)\n",
    "        if sum(labels) > len(labels)/2:\n",
    "            labels[labels==1] = -1\n",
    "            labels[labels==0] = 1\n",
    "            labels[labels==-1] = 0\n",
    "        return labels\n",
    "\n",
    "    def eval_metric(self,y_test, preds):\n",
    "        \"\"\" \n",
    "        print the classification report \n",
    "        \"\"\"\n",
    "        print('confusion matrix')\n",
    "        print(confusion_matrix(y_test, preds))\n",
    "        print('-------')\n",
    "     #   print('summary')\n",
    "        beta = 2\n",
    "        res = precision_recall_fscore_support(y_test, preds, beta = beta, pos_label = 1, average = 'binary')\n",
    "        print(\"precision:{}\\nrecall:{}\\nsupport:{}\".format(round(res[0],3),round(res[1],3),res[3]))\n",
    "        print('-------')\n",
    "        print(\"accuracy:\",round(accuracy_score(y_test, preds),3))\n",
    "        print(\"F-{} score:{}\".format(beta,round(res[2],3)))\n",
    "        print(\"\\n\")\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, preds, pos_label=2)\n",
    "        metrics.auc(fpr, tpr)   \n",
    "\n",
    "        \n",
    "    def model_eval(self,model,Normalize = False):\n",
    "        \"\"\"\n",
    "        print the evaluation result of the model with the Cp object \n",
    "        \"\"\"\n",
    "        self.data_preprocessing_0()\n",
    "        self.data_preprocessing_simple()\n",
    "        self.data_preprocessing_onehot()\n",
    "        \n",
    "        # one-hot\n",
    "        X_train, X_test, y_train, y_test = Cp.split_dataset(Cp.data_oh)\n",
    "        # one-hot [balanced]\n",
    "        X_train_b, X_test_b, y_train_b, y_test_b = Cp.split_dataset_4_imbal(Cp.data_oh)\n",
    "        # categorical -> 0:n_classes-1\n",
    "        X_train_t, X_test_t, y_train_t, y_test_t = Cp.split_dataset(Cp.data_simple)\n",
    "        # categorical -> 0:n_classes-1 [balanced]\n",
    "        X_train_tb, X_test_tb, y_train_tb, y_test_tb = Cp.split_dataset_4_imbal(Cp.data_simple)\n",
    "        \n",
    "        if Normalize:\n",
    "            X_train = normalize(X_train, norm='l2')\n",
    "            X_test = normalize(X_test, norm='l2')\n",
    "            \n",
    "        print('----------------one-hot-------------------')\n",
    "        model.fit(X_train, y_train)\n",
    "        pred_y = model.predict(X_test)\n",
    "        self.eval_metric(y_test, pred_y)\n",
    "        print('----------------one-hot[balanced]-------------------')\n",
    "        model.fit(X_train_b, y_train_b)\n",
    "        pred_y_b = model.predict(X_test_b)\n",
    "        self.eval_metric(y_test_b, pred_y_b)\n",
    "        print('----------------0:n_classes-1-------------------')\n",
    "        model.fit(X_train_t, y_train_t)\n",
    "        pred_y_t = model.predict(X_test_t)\n",
    "        Cp.eval_metric(y_test_t, pred_y_t)\n",
    "        print('----------------0:n_classes-1[balanced]-------------------')\n",
    "        model.fit(X_train_tb, y_train_tb)\n",
    "        pred_y_tb = model.predict(X_test_tb)\n",
    "        Cp.eval_metric(y_test_tb, pred_y_tb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'CreditTraining.csv'\n",
    "Cp = Credit_predictor(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Model selection \n",
    "### 3.1.1 LogisticRegression\n",
    "The *liblinear* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------one-hot-------------------\n",
      "confusion matrix\n",
      "[[1229    7]\n",
      " [  89   20]]\n",
      "-------\n",
      "precision:0.741\n",
      "recall:0.183\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.929\n",
      "F-2 score:0.216\n",
      "\n",
      "\n",
      "----------------one-hot[balanced]-------------------\n",
      "confusion matrix\n",
      "[[1109  127]\n",
      " [  16   93]]\n",
      "-------\n",
      "precision:0.423\n",
      "recall:0.853\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.894\n",
      "F-2 score:0.709\n",
      "\n",
      "\n",
      "----------------0:n_classes-1-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[1223   13]\n",
      " [  81   28]]\n",
      "-------\n",
      "precision:0.683\n",
      "recall:0.257\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.93\n",
      "F-2 score:0.294\n",
      "\n",
      "\n",
      "----------------0:n_classes-1[balanced]-------------------\n",
      "confusion matrix\n",
      "[[1112  124]\n",
      " [  16   93]]\n",
      "-------\n",
      "precision:0.429\n",
      "recall:0.853\n",
      "support:None\n",
      "-------\n",
      "accuracy: 0.896\n",
      "F-2 score:0.712\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/Users/onepear/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "Cp.model_eval(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=100)\n",
    "Cp.model_eval(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto',kernel='linear')\n",
    "Cp.model_eval(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "Cp.model_eval(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5. XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBClassifier(objective ='reg:logistic')\n",
    "model_eval(xg_reg, Cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_tweedie = xgb.XGBClassifier(objective ='binary:logitraw', eval_metric = 'error@0.5')\n",
    "model_eval(xg_tweedie, Cp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing objective and evaluation metric do not seem to have a huge difference.\n",
    "\n",
    "**Observations**\n",
    "- Balancing the data indeed helps to improve the model performance \n",
    "- We **select logistiRegression, LDA, and XGBoost given their $R_{\\text{label=1}}$ around 0.85, $P_{\\text{label=0}}=0.99$, $P_{\\text{label=1}}$ around 0.42 using the two balanced data.**\n",
    "\n",
    "\n",
    "## 3.2. Parameter Tuning \n",
    "\n",
    "In this section we perform more fine-grined parametr tuning for the three models mentioned.\n",
    "\n",
    "$\\color{red}{\\text{TODO:}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Voting  \n",
    "\n",
    "In this section, we propose two voting methods that utilize the predictions of several models and hopes it would maximize the information obatined so far.\n",
    "- Improve precision of predicted defaulted\n",
    "- Improve coverage of truly defaulted\n",
    "\n",
    "## 4.1. Minimize innocent (Improve precision of predicted defaulted)\n",
    "\n",
    "Since we have three models **logistiRegression, LDA, and XGBoost** using balanced data have $R_{\\text{label=1}}$ around 0.85, meaning most of the defaulted customers are detected (which we think is a **relative good coverage**).\n",
    "\n",
    "But $\\text{label=1} \\approx 0.42$ , meaning more than half of the predicted is not defaulted, and it would be costly to do a thorough investigation of the users so we want to **minimize those innocent but detected**.   \n",
    "\n",
    "With 3 predictions from 3 individual parties, we want to extract the mutual information and believing that if one client is predictd defaulted by all parties then there is a larger probability that the one is defaulted compared with (1/3 of the parties voted so). And of course, we might lose some covergae. **We try taking a logical AND to do this**.\n",
    "\n",
    "**Essentially, we are doing a voting by setting the weight to be 100% iff. all parties vote 1, ensuring that the number of inocent client is minimized**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_tb, y_train_tb)\n",
    "lr_pred_y_tb = lr.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y_tb)\n",
    "\n",
    "xg_reg.fit(X_train_tb, y_train_tb)\n",
    "xg_pred_y_tb = xg_reg.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, xg_pred_y_tb)\n",
    "\n",
    "LDA.fit(X_train_tb, y_train_tb)\n",
    "LDA_pred_y_tb = LDA.predict(X_test_tb)\n",
    "Cp.eval_metric(y_test_tb, LDA_pred_y_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cp.eval_metric(y_test_tb, LDA_pred_y_tb & xg_pred_y_tb & lr_pred_y_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: we did increase $P_{\\text{label=1}}$ to 0.44, but $R_{\\text{label=1}}$ drops to 0.84, the improvement of $P_{\\text{label=1}}$ is small, one potential explaination is that the three parties share a lot of mutual information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "parties_pred_mat = [LDA_pred_y_tb, xg_pred_y_tb, lr_pred_y_tb]\n",
    "distance.cdist(parties_pred_mat, parties_pred_mat, 'hamming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the *Minimize innocent* voting mechanism fail to improve a $P_{\\text{label=1}}$ a lot is largely due to the percentage of shared information is dominant for all three parties (around 1% difference).\n",
    "\n",
    "## 4.2. Add trustworthy information ( Improve coverage of truly defaulted)\n",
    "\n",
    "We use one of the three parties as a base (with good covergae) and then we add prediction by party with high $P_{\\text{label=1}}$ but possibly low coverage ($R_{\\text{label=1}}$), so as to improve precison and recall at the same time.\n",
    "\n",
    "**Essentially, we are doing a voting by setting the weight of trustworthy party to 100%, meaning once they vote 1, the client is 1 in prediction.**\n",
    "\n",
    "- Base\n",
    "    - `lr_pred_y_tb`: $P_{\\text{label=1}}=0.43, R_{\\text{label=1}}=0.88$ \n",
    "    - `xg_pred_y_tb`: $P_{\\text{label=1}}=0.43, R_{\\text{label=1}}=0.85$ \n",
    "- Trustworthy\n",
    "    - `lr_pred_y` : $P_{\\text{label=1}}=0.80, R_{\\text{label=1}}=0.15$ \n",
    "    - `rfc_pred_y` : $P_{\\text{label=1}}=0.76, R_{\\text{label=1}}=0.28$       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "rfc_pred_y = rfc.predict(X_test)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred_y = lr.predict(X_test)\n",
    "\n",
    "\n",
    "print('-------------------LDA+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | LDA_pred_y_tb)\n",
    "\n",
    "print('-------------------LDA+rfc-------------------')\n",
    "Cp.eval_metric(y_test_tb, rfc_pred_y | LDA_pred_y_tb)\n",
    "\n",
    "print('-------------------LDA+rfc+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | rfc_pred_y | LDA_pred_y_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------XG+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | xg_pred_y_tb)\n",
    "\n",
    "print('-------------------XG+rfc-------------------')\n",
    "Cp.eval_metric(y_test_tb, rfc_pred_y | xg_pred_y_tb)\n",
    "\n",
    "print('-------------------XG+rfc+lr-------------------')\n",
    "Cp.eval_metric(y_test_tb, lr_pred_y | rfc_pred_y | xg_pred_y_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, these *trustworthy information* seem to have been covered already by the parties with good coverage.\n",
    "\n",
    "# 5. Conclusion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
